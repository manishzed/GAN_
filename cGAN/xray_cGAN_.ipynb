{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/tvtaerum/xray_housekeeping/blob/master/files/images_convert_xrays.py"
      ],
      "metadata": {
        "id": "_rDcS-m13iJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n"
      ],
      "metadata": {
        "id": "7T1fcmc93iGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655b913c-3dc1-4d05-d095-041638c04eb9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (6.1.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2022.9.24)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->opendatasets) (3.0.4)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nQkP66mP3aCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94541c72-112c-46e0-f470-9a4c3b38991d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: softwebdatascience\n",
            "Your Kaggle Key: ··········\n",
            "Downloading chest-xray-pneumonia.zip to ./chest-xray-pneumonia\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.29G/2.29G [00:32<00:00, 75.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "import pandas\n",
        "  \n",
        "od.download(\"https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/xray/chest_xray/.DS_Store "
      ],
      "metadata": {
        "id": "NHBMP4YjSWsh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read and load data from folder"
      ],
      "metadata": {
        "id": "Cmf-1bEjTS_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of extracting and resizing xrays into a new dataset\n",
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from PIL import Image\n",
        "#from mtcnn.mtcnn import MTCNN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "np.set_printoptions(suppress=True)\n",
        "varNames = [\"NORMAL\",\"VIRAL\",\"BACTERIAL\"]\n",
        "\n",
        "# load an image as an rgb numpy array\n",
        "def load_image(filename):\n",
        "    # load image from file\n",
        "    image = Image.open(filename)\n",
        "    # convert to RGB, if needed\n",
        "    image = image.convert('RGB')\n",
        "    # convert to array\n",
        "    pixels = asarray(image)\n",
        "    return pixels\n",
        " \n",
        "# extract the xray from a loaded image and resize\n",
        "def extract_xray(pixels, required_size=(80, 80)):\n",
        "    # resize pixels to the model size\n",
        "    image = Image.fromarray(pixels)\n",
        "    image = image.resize(required_size)\n",
        "    xray_array = asarray(image)\n",
        "    return xray_array\n",
        " \n",
        "# load images and extract xrays for all images in a directory\n",
        "def load_xrays(directory, n_xrays):\n",
        "    xrays = list()\n",
        "    ids = list()\n",
        "    labels = list()\n",
        "    for dirname in listdir(directory):\n",
        "        directory1=directory+dirname\n",
        "        print(\"directory1: \", directory1)\n",
        "        for dirname1 in listdir(directory1):\n",
        "            directory2=directory1+\"/\"+dirname1\n",
        "            print(\"directory2: \", directory2)\n",
        "            # enumerate files\n",
        "            for idx, filename in enumerate(listdir(directory2)):\n",
        "                # load the image\n",
        "                if \"virus\" in filename: label=1\n",
        "                elif \"bacteria\" in filename: label=2\n",
        "                else: label=0\n",
        "                pixels = load_image(directory2 + \"/\" + filename)\n",
        "                # print(\"pixels.size: \", pixels.size)\n",
        "                # get xray\n",
        "                xray = extract_xray(pixels)\n",
        "                # print(\"pixels.size after extract: \", pixels.size)\n",
        "                # if xray is None:\n",
        "                    # continue\n",
        "                # if data_attractive[idx] == -1.0:\n",
        "                    # continue\n",
        "                # store\n",
        "                xrays.append(xray)\n",
        "                ids.append(idx)\n",
        "                labels.append(label)\n",
        "                if (len(xrays)+1)%100==0:\n",
        "                    print(len(xrays)+1, xray.shape)\n",
        "                # stop once we have enough\n",
        "                if len(xrays) >= n_xrays:\n",
        "                    break\n",
        "    return asarray(xrays),asarray(labels)\n",
        " \n",
        "# directory that contains all images\n",
        "directory = 'xray/chest_xray/'\n",
        "# load and extract all xrays\n",
        "n_xrays = 5200\n",
        "# n_xrays = 100\n",
        "all_xrays, all_labels = load_xrays(directory, n_xrays)\n",
        "print('Loaded xrays: ', all_xrays.shape)\n",
        "print('Loaded labels: ', all_labels.shape)\n",
        "qSave = True\n",
        "if qSave:\n",
        "    savez_compressed('xray/img_align_xray.npz', all_xrays)\n",
        "    savez_compressed('xray/labels_align_xray.npz', all_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eknd36aSoMMt",
        "outputId": "17f95b34-3937-4fbe-ac76-69927535ddab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "directory1:  xray/chest_xray/train\n",
            "directory2:  xray/chest_xray/train/PNEUMONIA\n",
            "100 (80, 80, 3)\n",
            "200 (80, 80, 3)\n",
            "300 (80, 80, 3)\n",
            "400 (80, 80, 3)\n",
            "500 (80, 80, 3)\n",
            "600 (80, 80, 3)\n",
            "700 (80, 80, 3)\n",
            "800 (80, 80, 3)\n",
            "900 (80, 80, 3)\n",
            "1000 (80, 80, 3)\n",
            "1100 (80, 80, 3)\n",
            "1200 (80, 80, 3)\n",
            "1300 (80, 80, 3)\n",
            "1400 (80, 80, 3)\n",
            "1500 (80, 80, 3)\n",
            "1600 (80, 80, 3)\n",
            "1700 (80, 80, 3)\n",
            "1800 (80, 80, 3)\n",
            "1900 (80, 80, 3)\n",
            "2000 (80, 80, 3)\n",
            "2100 (80, 80, 3)\n",
            "2200 (80, 80, 3)\n",
            "2300 (80, 80, 3)\n",
            "2400 (80, 80, 3)\n",
            "2500 (80, 80, 3)\n",
            "2600 (80, 80, 3)\n",
            "2700 (80, 80, 3)\n",
            "2800 (80, 80, 3)\n",
            "2900 (80, 80, 3)\n",
            "3000 (80, 80, 3)\n",
            "3100 (80, 80, 3)\n",
            "3200 (80, 80, 3)\n",
            "3300 (80, 80, 3)\n",
            "3400 (80, 80, 3)\n",
            "3500 (80, 80, 3)\n",
            "3600 (80, 80, 3)\n",
            "3700 (80, 80, 3)\n",
            "3800 (80, 80, 3)\n",
            "directory2:  xray/chest_xray/train/NORMAL\n",
            "3900 (80, 80, 3)\n",
            "4000 (80, 80, 3)\n",
            "4100 (80, 80, 3)\n",
            "4200 (80, 80, 3)\n",
            "4300 (80, 80, 3)\n",
            "4400 (80, 80, 3)\n",
            "4500 (80, 80, 3)\n",
            "4600 (80, 80, 3)\n",
            "4700 (80, 80, 3)\n",
            "4800 (80, 80, 3)\n",
            "4900 (80, 80, 3)\n",
            "5000 (80, 80, 3)\n",
            "5100 (80, 80, 3)\n",
            "5200 (80, 80, 3)\n",
            "Loaded xrays:  (5200, 80, 80, 3)\n",
            "Loaded labels:  (5200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -a /content/xray/chest_xray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wBrUIu9oMKl",
        "outputId": "cd8d54a9-455d-4021-c357-762ed3756d8f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".  ..  .DS_Store  train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing after loading .npz file "
      ],
      "metadata": {
        "id": "LZnha72hrG_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a gan for generating faces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy import append\n",
        "from numpy.random import random\n",
        "from numpy.random import randint\n",
        "from numpy.random import shuffle\n",
        "import time\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from matplotlib import patheffects as path_effects\n",
        "import collections\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow import get_logger as log\n",
        "\n",
        "\n",
        "def get_cumProbs(freqCategs, categs):\n",
        "    freqLists = [freqCategs[i][1] for i in range(len(freqCategs))]\n",
        "    freqListX = asarray(freqLists, dtype=np.float32)\n",
        "    print(\"freqListX: \", freqListX)\n",
        "    print(\"len(categs): \", len(categs))\n",
        "    cumProbs = freqListX/len(categs)\n",
        "    print(\"cumProbs: \", cumProbs)\n",
        "    cumProbs = append((0.0),cumProbs)\n",
        "    for i in range(len(cumProbs)-1):\n",
        "        cumProbs[i+1]=cumProbs[i]+cumProbs[i+1]\n",
        "    print(\"cumProbs: \", cumProbs)\n",
        "    return cumProbs\n",
        "\n",
        "\n",
        " \n",
        "# load and prepare training images\n",
        "def load_real_samples():\n",
        "    # load the face dataset\n",
        "    data = load('xray/img_align_xray.npz')\n",
        "    X = data['arr_0']\n",
        "    # convert from unsigned ints to floats\n",
        "    X = X.astype('float32')\n",
        "    # scale from [0,255] to [-1,1]\n",
        "    X = (X - 127.5) / 127.5\n",
        "    data  = load('xray/labels_align_xray.npz')\n",
        "    labels = data['arr_0']\n",
        "    print(\"labels: \", labels)\n",
        "    lenLabels = len(labels)\n",
        "    print(\"lenLabels: \", lenLabels)\n",
        "    lenrows = len(X)\n",
        "    freqCategs = list(collections.Counter(sorted(labels)).items())\n",
        "    print(\"freqCategs: \", freqCategs)\n",
        "    cumProbs = get_cumProbs(freqCategs, labels)\n",
        "    return [X, labels], cumProbs\n",
        "\n",
        "\n",
        "# load image data\n",
        "dataset, cumProbs = load_real_samples()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "    # print(\"n_samples: \", n_samples)\n",
        "    # split into images and labels\n",
        "    images, labels = dataset\n",
        "    #print(\"images, labels:\", images, labels)\n",
        "    # choose random instances\n",
        "    ix = randint(0, images.shape[0], n_samples)\n",
        "    # print(\"ix: \", ix)\n",
        "    # print(\"images.size: \", images.size)\n",
        "    # print(\"labels.size: \", labels.size)\n",
        "    # retrieve selected images\n",
        "    X, labels = images[ix], labels[ix]\n",
        "    print(\"X, labels\", X, labels)\n",
        "    # generate 'real' class labels (1)\n",
        "    y = ones((n_samples, 1))\n",
        "    return [X, labels], y\n",
        "\n",
        "\n",
        "\n",
        "varNames = [\"NORMAL\",\"VIRAL\",\"BACTERIAL\"]\n",
        "def save_real_plots(dataset, nRealPlots = 5, n=3, n_samples=100):\n",
        "    # plot images\n",
        "    for epoch in range(nRealPlots):\n",
        "        if epoch%5==0:\n",
        "            print(\"real_plots: \", epoch)\n",
        "        # prepare real samples\n",
        "        [X_real, labels], y_real = generate_real_samples(dataset, 100)\n",
        "        fig, ax = plt.subplots(n, n, figsize=(15,15))\n",
        "        ax=ax.flatten()\n",
        "        # scale from [-1,1] to [0,1]\n",
        "        X_real = (X_real + 1) / 2.0\n",
        "        for i in range(n * n):\n",
        "            # define subplot\n",
        "            #fig = plt.subplot(n, n, 1 + i)\n",
        "            strLabel = str(varNames[labels[i]])\n",
        "            print(\"strLabel :\", strLabel)\n",
        "            # fig.title = strLabel\n",
        "            # turn off axis\n",
        "            #fig.axis('off')\n",
        "            #fig.text(8.0,20.0,strLabel, fontsize=6, color='blue')\n",
        "            ax[i].set_title(strLabel)\n",
        "            # plot raw pixel data\n",
        "            ax[i].imshow(X_real[i])\n",
        "        # save plot to file\n",
        "        filename = 'xray/test_/real_plot_e%03d.png' % (epoch+1)\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        \n",
        "        \n",
        "        \n",
        "save_real_plots(dataset, nRealPlots=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82SXgu0OoMIE",
        "outputId": "e24a3de8-5a8a-4b17-e44d-5a6514256df2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  [2 1 1 ... 0 0 0]\n",
            "lenLabels:  5200\n",
            "freqCategs:  [(0, 1325), (1, 1345), (2, 2530)]\n",
            "freqListX:  [1325. 1345. 2530.]\n",
            "len(categs):  5200\n",
            "cumProbs:  [0.255 0.259 0.487]\n",
            "cumProbs:  [0.    0.255 0.513 1.   ]\n",
            "real_plots:  0\n",
            "X, labels [[[[-1.    -1.    -1.   ]\n",
            "   [-0.325 -0.325 -0.325]\n",
            "   [ 0.004  0.004  0.004]\n",
            "   ...\n",
            "   [ 0.208  0.208  0.208]\n",
            "   [ 0.129  0.129  0.129]\n",
            "   [-0.192 -0.192 -0.192]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.325 -0.325 -0.325]\n",
            "   [ 0.035  0.035  0.035]\n",
            "   ...\n",
            "   [ 0.192  0.192  0.192]\n",
            "   [ 0.114  0.114  0.114]\n",
            "   [-0.184 -0.184 -0.184]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.325 -0.325 -0.325]\n",
            "   [ 0.051  0.051  0.051]\n",
            "   ...\n",
            "   [ 0.2    0.2    0.2  ]\n",
            "   [ 0.129  0.129  0.129]\n",
            "   [-0.192 -0.192 -0.192]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.741 -0.741 -0.741]\n",
            "   [-0.49  -0.49  -0.49 ]\n",
            "   [-0.278 -0.278 -0.278]\n",
            "   ...\n",
            "   [-0.89  -0.89  -0.89 ]\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.718 -0.718 -0.718]\n",
            "   [-0.49  -0.49  -0.49 ]\n",
            "   [-0.286 -0.286 -0.286]\n",
            "   ...\n",
            "   [-0.882 -0.882 -0.882]\n",
            "   [-0.961 -0.961 -0.961]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.71  -0.71  -0.71 ]\n",
            "   [-0.482 -0.482 -0.482]\n",
            "   [-0.302 -0.302 -0.302]\n",
            "   ...\n",
            "   [-0.882 -0.882 -0.882]\n",
            "   [-0.961 -0.961 -0.961]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[-0.318 -0.318 -0.318]\n",
            "   [-0.373 -0.373 -0.373]\n",
            "   [-0.38  -0.38  -0.38 ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.184 -0.184 -0.184]\n",
            "   [ 0.239  0.239  0.239]\n",
            "   [-0.663 -0.663 -0.663]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.263 -0.263 -0.263]\n",
            "   [-0.114 -0.114 -0.114]\n",
            "   [-0.459 -0.459 -0.459]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.961 -0.961 -0.961]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[-0.137 -0.137 -0.137]\n",
            "   [-0.106 -0.106 -0.106]\n",
            "   [-0.067 -0.067 -0.067]\n",
            "   ...\n",
            "   [-0.514 -0.514 -0.514]\n",
            "   [-0.584 -0.584 -0.584]\n",
            "   [-0.686 -0.686 -0.686]]\n",
            "\n",
            "  [[-0.686 -0.686 -0.686]\n",
            "   [-0.749 -0.749 -0.749]\n",
            "   [-0.655 -0.655 -0.655]\n",
            "   ...\n",
            "   [-0.608 -0.608 -0.608]\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.78  -0.78  -0.78 ]]\n",
            "\n",
            "  [[-0.827 -0.827 -0.827]\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   [-0.835 -0.835 -0.835]\n",
            "   ...\n",
            "   [-0.624 -0.624 -0.624]\n",
            "   [-0.694 -0.694 -0.694]\n",
            "   [-0.82  -0.82  -0.82 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.067 -0.067 -0.067]\n",
            "   [-0.027 -0.027 -0.027]\n",
            "   [ 0.075  0.075  0.075]\n",
            "   ...\n",
            "   [ 0.082  0.082  0.082]\n",
            "   [ 0.059  0.059  0.059]\n",
            "   [ 0.067  0.067  0.067]]\n",
            "\n",
            "  [[-0.067 -0.067 -0.067]\n",
            "   [-0.027 -0.027 -0.027]\n",
            "   [ 0.082  0.082  0.082]\n",
            "   ...\n",
            "   [ 0.082  0.082  0.082]\n",
            "   [ 0.075  0.075  0.075]\n",
            "   [ 0.114  0.114  0.114]]\n",
            "\n",
            "  [[-0.067 -0.067 -0.067]\n",
            "   [-0.027 -0.027 -0.027]\n",
            "   [ 0.059  0.059  0.059]\n",
            "   ...\n",
            "   [ 0.09   0.09   0.09 ]\n",
            "   [ 0.106  0.106  0.106]\n",
            "   [ 0.153  0.153  0.153]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.161 -0.161 -0.161]\n",
            "   [-0.059 -0.059 -0.059]\n",
            "   [ 0.067  0.067  0.067]\n",
            "   ...\n",
            "   [ 0.106  0.106  0.106]\n",
            "   [-0.09  -0.09  -0.09 ]\n",
            "   [-0.333 -0.333 -0.333]]\n",
            "\n",
            "  [[-0.153 -0.153 -0.153]\n",
            "   [-0.051 -0.051 -0.051]\n",
            "   [ 0.082  0.082  0.082]\n",
            "   ...\n",
            "   [ 0.067  0.067  0.067]\n",
            "   [-0.106 -0.106 -0.106]\n",
            "   [-0.333 -0.333 -0.333]]\n",
            "\n",
            "  [[-0.145 -0.145 -0.145]\n",
            "   [-0.035 -0.035 -0.035]\n",
            "   [ 0.082  0.082  0.082]\n",
            "   ...\n",
            "   [ 0.051  0.051  0.051]\n",
            "   [-0.106 -0.106 -0.106]\n",
            "   [-0.349 -0.349 -0.349]]]\n",
            "\n",
            "\n",
            " [[[-0.867 -0.867 -0.867]\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   [-0.882 -0.882 -0.882]\n",
            "   ...\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.835 -0.835 -0.835]\n",
            "   [-0.835 -0.835 -0.835]]\n",
            "\n",
            "  [[-0.882 -0.882 -0.882]\n",
            "   [-0.898 -0.898 -0.898]\n",
            "   [-0.906 -0.906 -0.906]\n",
            "   ...\n",
            "   [-0.843 -0.843 -0.843]\n",
            "   [-0.843 -0.843 -0.843]\n",
            "   [-0.835 -0.835 -0.835]]\n",
            "\n",
            "  [[-0.89  -0.89  -0.89 ]\n",
            "   [-0.937 -0.937 -0.937]\n",
            "   [-0.937 -0.937 -0.937]\n",
            "   ...\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   [-0.82  -0.82  -0.82 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 0.576  0.576  0.576]\n",
            "   [ 0.553  0.553  0.553]\n",
            "   [ 0.545  0.545  0.545]\n",
            "   ...\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.788 -0.788 -0.788]]\n",
            "\n",
            "  [[ 0.529  0.529  0.529]\n",
            "   [ 0.553  0.553  0.553]\n",
            "   [ 0.537  0.537  0.537]\n",
            "   ...\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.733 -0.733 -0.733]]\n",
            "\n",
            "  [[ 0.576  0.576  0.576]\n",
            "   [ 0.553  0.553  0.553]\n",
            "   [ 0.545  0.545  0.545]\n",
            "   ...\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.796 -0.796 -0.796]]]\n",
            "\n",
            "\n",
            " [[[-0.976 -0.976 -0.976]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   [-0.976 -0.976 -0.976]\n",
            "   [-0.984 -0.984 -0.984]]\n",
            "\n",
            "  [[-0.976 -0.976 -0.976]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   [-0.945 -0.945 -0.945]\n",
            "   [-0.953 -0.953 -0.953]]\n",
            "\n",
            "  [[-0.969 -0.969 -0.969]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   [-0.937 -0.937 -0.937]\n",
            "   [-0.945 -0.945 -0.945]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-0.992 -0.992 -0.992]]]] [2 1 0 1 0 2 0 2 2 2 1 1 2 2 2 0 1 1 0 1 2 1 0 2 1 2 0 2 0 2 2 2 0 2 2 2 1\n",
            " 0 2 1 2 0 1 2 0 2 2 1 0 1 2 1 1 2 2 0 2 0 1 1 1 0 2 2 0 0 1 0 2 0 2 2 0 2\n",
            " 0 2 2 2 2 1 0 2 0 2 0 2 0 2 2 0 1 2 1 0 2 2 2 2 2 0]\n",
            "strLabel : BACTERIAL\n",
            "strLabel : VIRAL\n",
            "strLabel : NORMAL\n",
            "strLabel : VIRAL\n",
            "strLabel : NORMAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : NORMAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "X, labels [[[[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[-0.584 -0.584 -0.584]\n",
            "   [-0.412 -0.412 -0.412]\n",
            "   [-0.318 -0.318 -0.318]\n",
            "   ...\n",
            "   [-0.302 -0.302 -0.302]\n",
            "   [-0.482 -0.482 -0.482]\n",
            "   [-0.71  -0.71  -0.71 ]]\n",
            "\n",
            "  [[-0.576 -0.576 -0.576]\n",
            "   [-0.42  -0.42  -0.42 ]\n",
            "   [-0.318 -0.318 -0.318]\n",
            "   ...\n",
            "   [-0.318 -0.318 -0.318]\n",
            "   [-0.49  -0.49  -0.49 ]\n",
            "   [-0.718 -0.718 -0.718]]\n",
            "\n",
            "  [[-0.576 -0.576 -0.576]\n",
            "   [-0.42  -0.42  -0.42 ]\n",
            "   [-0.318 -0.318 -0.318]\n",
            "   ...\n",
            "   [-0.325 -0.325 -0.325]\n",
            "   [-0.498 -0.498 -0.498]\n",
            "   [-0.733 -0.733 -0.733]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[-0.694 -0.694 -0.694]\n",
            "   [-0.561 -0.561 -0.561]\n",
            "   [-0.467 -0.467 -0.467]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.725 -0.725 -0.725]\n",
            "   [-0.6   -0.6   -0.6  ]\n",
            "   [-0.482 -0.482 -0.482]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.733 -0.733 -0.733]\n",
            "   [-0.631 -0.631 -0.631]\n",
            "   [-0.49  -0.49  -0.49 ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.984 -0.984 -0.984]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  [[-0.953 -0.953 -0.953]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.953 -0.953 -0.953]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.482 -0.482 -0.482]\n",
            "   [-0.482 -0.482 -0.482]\n",
            "   [-0.467 -0.467 -0.467]\n",
            "   ...\n",
            "   [-0.561 -0.561 -0.561]\n",
            "   [-0.631 -0.631 -0.631]\n",
            "   [-0.647 -0.647 -0.647]]\n",
            "\n",
            "  [[-0.49  -0.49  -0.49 ]\n",
            "   [-0.498 -0.498 -0.498]\n",
            "   [-0.498 -0.498 -0.498]\n",
            "   ...\n",
            "   [-0.576 -0.576 -0.576]\n",
            "   [-0.663 -0.663 -0.663]\n",
            "   [-0.741 -0.741 -0.741]]\n",
            "\n",
            "  [[-0.514 -0.514 -0.514]\n",
            "   [-0.514 -0.514 -0.514]\n",
            "   [-0.506 -0.506 -0.506]\n",
            "   ...\n",
            "   [-0.608 -0.608 -0.608]\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   [-0.812 -0.812 -0.812]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   ...\n",
            "   [-0.929 -0.929 -0.929]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   ...\n",
            "   [-0.898 -0.898 -0.898]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   ...\n",
            "   [-0.859 -0.859 -0.859]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[ 0.059  0.059  0.059]\n",
            "   [-0.145 -0.145 -0.145]\n",
            "   [-0.725 -0.725 -0.725]\n",
            "   ...\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  [[ 0.012  0.012  0.012]\n",
            "   [-0.224 -0.224 -0.224]\n",
            "   [-0.757 -0.757 -0.757]\n",
            "   ...\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   [-0.89  -0.89  -0.89 ]\n",
            "   [-0.922 -0.922 -0.922]]\n",
            "\n",
            "  [[-0.004 -0.004 -0.004]\n",
            "   [-0.31  -0.31  -0.31 ]\n",
            "   [-0.788 -0.788 -0.788]\n",
            "   ...\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.89  -0.89  -0.89 ]\n",
            "   [-0.867 -0.867 -0.867]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.459 -0.459 -0.459]\n",
            "   [-0.522 -0.522 -0.522]\n",
            "   [-0.569 -0.569 -0.569]\n",
            "   ...\n",
            "   [-0.671 -0.671 -0.671]\n",
            "   [-0.71  -0.71  -0.71 ]\n",
            "   [-0.71  -0.71  -0.71 ]]\n",
            "\n",
            "  [[-0.467 -0.467 -0.467]\n",
            "   [-0.537 -0.537 -0.537]\n",
            "   [-0.592 -0.592 -0.592]\n",
            "   ...\n",
            "   [-0.686 -0.686 -0.686]\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   [-0.718 -0.718 -0.718]]\n",
            "\n",
            "  [[-0.475 -0.475 -0.475]\n",
            "   [-0.561 -0.561 -0.561]\n",
            "   [-0.631 -0.631 -0.631]\n",
            "   ...\n",
            "   [-0.694 -0.694 -0.694]\n",
            "   [-0.71  -0.71  -0.71 ]\n",
            "   [-0.71  -0.71  -0.71 ]]]\n",
            "\n",
            "\n",
            " [[[-0.945 -0.945 -0.945]\n",
            "   [-0.937 -0.937 -0.937]\n",
            "   [-0.929 -0.929 -0.929]\n",
            "   ...\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   [-0.976 -0.976 -0.976]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.953 -0.953 -0.953]\n",
            "   [-0.937 -0.937 -0.937]\n",
            "   [-0.929 -0.929 -0.929]\n",
            "   ...\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  [[-0.961 -0.961 -0.961]\n",
            "   [-0.945 -0.945 -0.945]\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   ...\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.945 -0.945 -0.945]\n",
            "   [-0.976 -0.976 -0.976]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.922 -0.922 -0.922]\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   [-0.929 -0.929 -0.929]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.914 -0.914 -0.914]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.914 -0.914 -0.914]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]]]] [0 0 2 2 2 1 2 0 2 1 2 2 1 2 0 2 2 1 2 0 1 2 2 2 0 2 1 0 2 2 2 2 1 2 2 1 1\n",
            " 1 0 1 1 0 2 0 0 1 1 2 0 2 2 1 1 0 2 0 0 0 0 1 1 1 2 2 0 0 2 2 2 2 2 2 0 2\n",
            " 0 2 2 2 2 2 1 1 1 2 2 2 2 0 1 1 0 2 0 1 0 2 1 1 2 2]\n",
            "strLabel : NORMAL\n",
            "strLabel : NORMAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : VIRAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : NORMAL\n",
            "strLabel : BACTERIAL\n",
            "X, labels [[[[-0.082 -0.082 -0.082]\n",
            "   [ 0.106  0.106  0.106]\n",
            "   [ 0.341  0.341  0.341]\n",
            "   ...\n",
            "   [-0.271 -0.271 -0.271]\n",
            "   [-0.012 -0.012 -0.012]\n",
            "   [-0.255 -0.255 -0.255]]\n",
            "\n",
            "  [[-0.035 -0.035 -0.035]\n",
            "   [ 0.075  0.075  0.075]\n",
            "   [ 0.435  0.435  0.435]\n",
            "   ...\n",
            "   [-0.42  -0.42  -0.42 ]\n",
            "   [-0.137 -0.137 -0.137]\n",
            "   [-0.216 -0.216 -0.216]]\n",
            "\n",
            "  [[-0.082 -0.082 -0.082]\n",
            "   [ 0.114  0.114  0.114]\n",
            "   [ 0.412  0.412  0.412]\n",
            "   ...\n",
            "   [-0.514 -0.514 -0.514]\n",
            "   [-0.349 -0.349 -0.349]\n",
            "   [-0.216 -0.216 -0.216]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[-0.529 -0.529 -0.529]\n",
            "   [-0.553 -0.553 -0.553]\n",
            "   [-0.522 -0.522 -0.522]\n",
            "   ...\n",
            "   [-0.498 -0.498 -0.498]\n",
            "   [-0.522 -0.522 -0.522]\n",
            "   [-0.522 -0.522 -0.522]]\n",
            "\n",
            "  [[-0.498 -0.498 -0.498]\n",
            "   [-0.545 -0.545 -0.545]\n",
            "   [-0.537 -0.537 -0.537]\n",
            "   ...\n",
            "   [-0.498 -0.498 -0.498]\n",
            "   [-0.522 -0.522 -0.522]\n",
            "   [-0.529 -0.529 -0.529]]\n",
            "\n",
            "  [[-0.514 -0.514 -0.514]\n",
            "   [-0.553 -0.553 -0.553]\n",
            "   [-0.545 -0.545 -0.545]\n",
            "   ...\n",
            "   [-0.482 -0.482 -0.482]\n",
            "   [-0.522 -0.522 -0.522]\n",
            "   [-0.537 -0.537 -0.537]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.569 -0.569 -0.569]\n",
            "   [-0.537 -0.537 -0.537]\n",
            "   [-0.412 -0.412 -0.412]\n",
            "   ...\n",
            "   [-0.49  -0.49  -0.49 ]\n",
            "   [-0.545 -0.545 -0.545]\n",
            "   [-0.576 -0.576 -0.576]]\n",
            "\n",
            "  [[-0.576 -0.576 -0.576]\n",
            "   [-0.537 -0.537 -0.537]\n",
            "   [-0.412 -0.412 -0.412]\n",
            "   ...\n",
            "   [-0.49  -0.49  -0.49 ]\n",
            "   [-0.553 -0.553 -0.553]\n",
            "   [-0.569 -0.569 -0.569]]\n",
            "\n",
            "  [[-0.584 -0.584 -0.584]\n",
            "   [-0.537 -0.537 -0.537]\n",
            "   [-0.412 -0.412 -0.412]\n",
            "   ...\n",
            "   [-0.482 -0.482 -0.482]\n",
            "   [-0.553 -0.553 -0.553]\n",
            "   [-0.569 -0.569 -0.569]]]\n",
            "\n",
            "\n",
            " [[[-1.    -1.    -1.   ]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   ...\n",
            "   [-0.725 -0.725 -0.725]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.867 -0.867 -0.867]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   ...\n",
            "   [-0.702 -0.702 -0.702]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   [-0.906 -0.906 -0.906]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-0.624 -0.624 -0.624]\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   [-0.945 -0.945 -0.945]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.357 -0.357 -0.357]\n",
            "   [-0.71  -0.71  -0.71 ]\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   ...\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   [-0.812 -0.812 -0.812]]\n",
            "\n",
            "  [[-0.357 -0.357 -0.357]\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.788 -0.788 -0.788]\n",
            "   ...\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   [-0.82  -0.82  -0.82 ]]\n",
            "\n",
            "  [[-0.349 -0.349 -0.349]\n",
            "   [-0.686 -0.686 -0.686]\n",
            "   [-0.788 -0.788 -0.788]\n",
            "   ...\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   [-0.796 -0.796 -0.796]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.278 -0.278 -0.278]\n",
            "   [-0.396 -0.396 -0.396]\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   ...\n",
            "   [-0.757 -0.757 -0.757]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.773 -0.773 -0.773]]\n",
            "\n",
            "  [[-0.42  -0.42  -0.42 ]\n",
            "   [-0.569 -0.569 -0.569]\n",
            "   [-0.6   -0.6   -0.6  ]\n",
            "   ...\n",
            "   [-0.749 -0.749 -0.749]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.773 -0.773 -0.773]]\n",
            "\n",
            "  [[-0.435 -0.435 -0.435]\n",
            "   [-0.702 -0.702 -0.702]\n",
            "   [-0.506 -0.506 -0.506]\n",
            "   ...\n",
            "   [-0.749 -0.749 -0.749]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.78  -0.78  -0.78 ]]]\n",
            "\n",
            "\n",
            " [[[-0.522 -0.522 -0.522]\n",
            "   [-0.624 -0.624 -0.624]\n",
            "   [-0.576 -0.576 -0.576]\n",
            "   ...\n",
            "   [-0.451 -0.451 -0.451]\n",
            "   [-0.059 -0.059 -0.059]\n",
            "   [-0.137 -0.137 -0.137]]\n",
            "\n",
            "  [[-0.537 -0.537 -0.537]\n",
            "   [-0.631 -0.631 -0.631]\n",
            "   [-0.467 -0.467 -0.467]\n",
            "   ...\n",
            "   [-0.435 -0.435 -0.435]\n",
            "   [-0.09  -0.09  -0.09 ]\n",
            "   [-0.161 -0.161 -0.161]]\n",
            "\n",
            "  [[-0.349 -0.349 -0.349]\n",
            "   [-0.427 -0.427 -0.427]\n",
            "   [-0.231 -0.231 -0.231]\n",
            "   ...\n",
            "   [-0.404 -0.404 -0.404]\n",
            "   [-0.145 -0.145 -0.145]\n",
            "   [-0.192 -0.192 -0.192]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[-0.835 -0.835 -0.835]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.569 -0.569 -0.569]\n",
            "   ...\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.796 -0.796 -0.796]]\n",
            "\n",
            "  [[-0.812 -0.812 -0.812]\n",
            "   [-0.6   -0.6   -0.6  ]\n",
            "   [-0.529 -0.529 -0.529]\n",
            "   ...\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.804 -0.804 -0.804]]\n",
            "\n",
            "  [[-0.631 -0.631 -0.631]\n",
            "   [-0.545 -0.545 -0.545]\n",
            "   [-0.522 -0.522 -0.522]\n",
            "   ...\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   [-0.804 -0.804 -0.804]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.671 -0.671 -0.671]\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.702 -0.702 -0.702]\n",
            "   ...\n",
            "   [-0.655 -0.655 -0.655]\n",
            "   [-0.647 -0.647 -0.647]\n",
            "   [-0.647 -0.647 -0.647]]\n",
            "\n",
            "  [[-0.671 -0.671 -0.671]\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.702 -0.702 -0.702]\n",
            "   ...\n",
            "   [-0.655 -0.655 -0.655]\n",
            "   [-0.647 -0.647 -0.647]\n",
            "   [-0.647 -0.647 -0.647]]\n",
            "\n",
            "  [[-0.663 -0.663 -0.663]\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.702 -0.702 -0.702]\n",
            "   ...\n",
            "   [-0.655 -0.655 -0.655]\n",
            "   [-0.647 -0.647 -0.647]\n",
            "   [-0.647 -0.647 -0.647]]]] [1 2 0 2 0 1 0 2 2 2 0 0 0 1 2 2 2 2 2 2 0 1 1 0 1 2 0 2 1 1 2 0 2 2 2 2 2\n",
            " 0 1 0 2 0 0 1 0 0 2 2 2 1 2 0 1 0 0 2 2 2 0 0 2 2 2 1 1 1 1 0 1 2 2 2 1 0\n",
            " 1 0 1 2 0 2 1 2 0 0 2 1 2 1 2 2 0 2 1 2 0 2 1 2 1 2]\n",
            "strLabel : VIRAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : NORMAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : NORMAL\n",
            "strLabel : VIRAL\n",
            "strLabel : NORMAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "X, labels [[[[-0.867 -0.867 -0.867]\n",
            "   [-0.882 -0.882 -0.882]\n",
            "   [-0.906 -0.906 -0.906]\n",
            "   ...\n",
            "   [-0.514 -0.514 -0.514]\n",
            "   [-0.467 -0.467 -0.467]\n",
            "   [-0.114 -0.114 -0.114]]\n",
            "\n",
            "  [[-0.882 -0.882 -0.882]\n",
            "   [-0.89  -0.89  -0.89 ]\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   ...\n",
            "   [-0.49  -0.49  -0.49 ]\n",
            "   [-0.451 -0.451 -0.451]\n",
            "   [-0.216 -0.216 -0.216]]\n",
            "\n",
            "  [[-0.867 -0.867 -0.867]\n",
            "   [-0.741 -0.741 -0.741]\n",
            "   [-0.686 -0.686 -0.686]\n",
            "   ...\n",
            "   [-0.475 -0.475 -0.475]\n",
            "   [-0.459 -0.459 -0.459]\n",
            "   [-0.247 -0.247 -0.247]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.725 -0.725 -0.725]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   ...\n",
            "   [-0.733 -0.733 -0.733]\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   [-0.78  -0.78  -0.78 ]]\n",
            "\n",
            "  [[-0.773 -0.773 -0.773]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   ...\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.835 -0.835 -0.835]\n",
            "   [-0.835 -0.835 -0.835]]\n",
            "\n",
            "  [[-0.78  -0.78  -0.78 ]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.788 -0.788 -0.788]\n",
            "   ...\n",
            "   [-0.671 -0.671 -0.671]\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.843 -0.843 -0.843]]]\n",
            "\n",
            "\n",
            " [[[-0.435 -0.435 -0.435]\n",
            "   [-0.42  -0.42  -0.42 ]\n",
            "   [-0.396 -0.396 -0.396]\n",
            "   ...\n",
            "   [-0.89  -0.89  -0.89 ]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.318 -0.318 -0.318]\n",
            "   [-0.373 -0.373 -0.373]\n",
            "   [-0.38  -0.38  -0.38 ]\n",
            "   ...\n",
            "   [-0.929 -0.929 -0.929]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.451 -0.451 -0.451]\n",
            "   [-0.278 -0.278 -0.278]\n",
            "   [-0.349 -0.349 -0.349]\n",
            "   ...\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-0.969 -0.969 -0.969]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  [[-0.953 -0.953 -0.953]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.953 -0.953 -0.953]]]\n",
            "\n",
            "\n",
            " [[[-0.812 -0.812 -0.812]\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   [-0.898 -0.898 -0.898]\n",
            "   ...\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.835 -0.835 -0.835]]\n",
            "\n",
            "  [[-0.843 -0.843 -0.843]\n",
            "   [-0.859 -0.859 -0.859]\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   ...\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.827 -0.827 -0.827]]\n",
            "\n",
            "  [[-0.827 -0.827 -0.827]\n",
            "   [-0.898 -0.898 -0.898]\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   ...\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.804 -0.804 -0.804]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.851 -0.851 -0.851]\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   [-0.906 -0.906 -0.906]\n",
            "   ...\n",
            "   [-0.671 -0.671 -0.671]\n",
            "   [-0.631 -0.631 -0.631]\n",
            "   [-0.624 -0.624 -0.624]]\n",
            "\n",
            "  [[-0.843 -0.843 -0.843]\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   ...\n",
            "   [-0.725 -0.725 -0.725]\n",
            "   [-0.686 -0.686 -0.686]\n",
            "   [-0.663 -0.663 -0.663]]\n",
            "\n",
            "  [[-0.82  -0.82  -0.82 ]\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   ...\n",
            "   [-0.757 -0.757 -0.757]\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   [-0.671 -0.671 -0.671]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 0.365  0.365  0.365]\n",
            "   [ 0.278  0.278  0.278]\n",
            "   [ 0.153  0.153  0.153]\n",
            "   ...\n",
            "   [ 0.584  0.584  0.584]\n",
            "   [ 0.616  0.616  0.616]\n",
            "   [ 0.655  0.655  0.655]]\n",
            "\n",
            "  [[ 0.247  0.247  0.247]\n",
            "   [ 0.216  0.216  0.216]\n",
            "   [ 0.082  0.082  0.082]\n",
            "   ...\n",
            "   [ 0.569  0.569  0.569]\n",
            "   [ 0.608  0.608  0.608]\n",
            "   [ 0.647  0.647  0.647]]\n",
            "\n",
            "  [[ 0.239  0.239  0.239]\n",
            "   [ 0.192  0.192  0.192]\n",
            "   [ 0.051  0.051  0.051]\n",
            "   ...\n",
            "   [ 0.569  0.569  0.569]\n",
            "   [ 0.592  0.592  0.592]\n",
            "   [ 0.624  0.624  0.624]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.835 -0.835 -0.835]\n",
            "   [-0.843 -0.843 -0.843]\n",
            "   [-0.859 -0.859 -0.859]\n",
            "   ...\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.804 -0.804 -0.804]]\n",
            "\n",
            "  [[-0.835 -0.835 -0.835]\n",
            "   [-0.843 -0.843 -0.843]\n",
            "   [-0.867 -0.867 -0.867]\n",
            "   ...\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.804 -0.804 -0.804]]\n",
            "\n",
            "  [[-0.835 -0.835 -0.835]\n",
            "   [-0.843 -0.843 -0.843]\n",
            "   [-0.867 -0.867 -0.867]\n",
            "   ...\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.804 -0.804 -0.804]]]\n",
            "\n",
            "\n",
            " [[[ 0.545  0.545  0.545]\n",
            "   [ 0.584  0.584  0.584]\n",
            "   [ 0.553  0.553  0.553]\n",
            "   ...\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   [-0.631 -0.631 -0.631]]\n",
            "\n",
            "  [[-0.286 -0.286 -0.286]\n",
            "   [-0.216 -0.216 -0.216]\n",
            "   [-0.184 -0.184 -0.184]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.773 -0.773 -0.773]\n",
            "   [-0.702 -0.702 -0.702]\n",
            "   [-0.624 -0.624 -0.624]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [ 1.     1.     1.   ]\n",
            "   [ 1.     1.     1.   ]\n",
            "   [ 1.     1.     1.   ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [ 1.     1.     1.   ]\n",
            "   [ 1.     1.     1.   ]\n",
            "   [ 0.992  0.992  0.992]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [ 1.     1.     1.   ]\n",
            "   [ 1.     1.     1.   ]\n",
            "   [ 0.992  0.992  0.992]]]\n",
            "\n",
            "\n",
            " [[[-0.773 -0.773 -0.773]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   ...\n",
            "   [-0.357 -0.357 -0.357]\n",
            "   [-0.396 -0.396 -0.396]\n",
            "   [-0.404 -0.404 -0.404]]\n",
            "\n",
            "  [[-0.773 -0.773 -0.773]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   ...\n",
            "   [-0.365 -0.365 -0.365]\n",
            "   [-0.388 -0.388 -0.388]\n",
            "   [-0.412 -0.412 -0.412]]\n",
            "\n",
            "  [[-0.78  -0.78  -0.78 ]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   ...\n",
            "   [-0.365 -0.365 -0.365]\n",
            "   [-0.404 -0.404 -0.404]\n",
            "   [-0.443 -0.443 -0.443]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.953 -0.953 -0.953]\n",
            "   [-0.827 -0.827 -0.827]\n",
            "   ...\n",
            "   [-0.145 -0.145 -0.145]\n",
            "   [-0.231 -0.231 -0.231]\n",
            "   [-0.6   -0.6   -0.6  ]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.929 -0.929 -0.929]\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   ...\n",
            "   [-0.169 -0.169 -0.169]\n",
            "   [-0.239 -0.239 -0.239]\n",
            "   [-0.584 -0.584 -0.584]]\n",
            "\n",
            "  [[-1.    -1.    -1.   ]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   ...\n",
            "   [-0.216 -0.216 -0.216]\n",
            "   [-0.239 -0.239 -0.239]\n",
            "   [-0.569 -0.569 -0.569]]]] [2 1 2 0 2 2 2 1 0 2 0 0 2 0 1 2 2 2 0 1 2 0 1 2 0 2 2 0 0 2 0 0 2 2 2 1 0\n",
            " 2 1 2 2 2 2 1 1 0 2 2 2 2 2 1 0 0 2 1 2 0 1 2 2 2 0 0 2 1 1 2 0 1 2 0 2 2\n",
            " 0 2 0 2 0 1 2 0 0 2 2 0 1 0 2 0 2 1 2 1 1 0 1 2 0 2]\n",
            "strLabel : BACTERIAL\n",
            "strLabel : VIRAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : NORMAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : VIRAL\n",
            "strLabel : NORMAL\n",
            "X, labels [[[[-0.749 -0.749 -0.749]\n",
            "   [ 0.075  0.075  0.075]\n",
            "   [ 0.122  0.122  0.122]\n",
            "   ...\n",
            "   [-0.145 -0.145 -0.145]\n",
            "   [-0.247 -0.247 -0.247]\n",
            "   [-0.42  -0.42  -0.42 ]]\n",
            "\n",
            "  [[-0.749 -0.749 -0.749]\n",
            "   [ 0.067  0.067  0.067]\n",
            "   [ 0.106  0.106  0.106]\n",
            "   ...\n",
            "   [-0.153 -0.153 -0.153]\n",
            "   [-0.263 -0.263 -0.263]\n",
            "   [-0.443 -0.443 -0.443]]\n",
            "\n",
            "  [[-0.757 -0.757 -0.757]\n",
            "   [ 0.043  0.043  0.043]\n",
            "   [ 0.082  0.082  0.082]\n",
            "   ...\n",
            "   [-0.169 -0.169 -0.169]\n",
            "   [-0.286 -0.286 -0.286]\n",
            "   [-0.467 -0.467 -0.467]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.812 -0.812 -0.812]\n",
            "   [-0.498 -0.498 -0.498]\n",
            "   [-0.271 -0.271 -0.271]\n",
            "   ...\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.898 -0.898 -0.898]\n",
            "   [-0.992 -0.992 -0.992]]\n",
            "\n",
            "  [[-0.796 -0.796 -0.796]\n",
            "   [-0.459 -0.459 -0.459]\n",
            "   [-0.247 -0.247 -0.247]\n",
            "   ...\n",
            "   [-0.749 -0.749 -0.749]\n",
            "   [-0.906 -0.906 -0.906]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.765 -0.765 -0.765]\n",
            "   [-0.427 -0.427 -0.427]\n",
            "   [-0.231 -0.231 -0.231]\n",
            "   ...\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   [-0.906 -0.906 -0.906]\n",
            "   [-1.    -1.    -1.   ]]]\n",
            "\n",
            "\n",
            " [[[-0.412 -0.412 -0.412]\n",
            "   [-0.373 -0.373 -0.373]\n",
            "   [-0.357 -0.357 -0.357]\n",
            "   ...\n",
            "   [-0.435 -0.435 -0.435]\n",
            "   [-0.467 -0.467 -0.467]\n",
            "   [-0.514 -0.514 -0.514]]\n",
            "\n",
            "  [[-0.38  -0.38  -0.38 ]\n",
            "   [-0.357 -0.357 -0.357]\n",
            "   [-0.333 -0.333 -0.333]\n",
            "   ...\n",
            "   [-0.396 -0.396 -0.396]\n",
            "   [-0.42  -0.42  -0.42 ]\n",
            "   [-0.459 -0.459 -0.459]]\n",
            "\n",
            "  [[-0.341 -0.341 -0.341]\n",
            "   [-0.341 -0.341 -0.341]\n",
            "   [-0.325 -0.325 -0.325]\n",
            "   ...\n",
            "   [-0.38  -0.38  -0.38 ]\n",
            "   [-0.404 -0.404 -0.404]\n",
            "   [-0.42  -0.42  -0.42 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.78  -0.78  -0.78 ]\n",
            "   [-0.78  -0.78  -0.78 ]\n",
            "   [-0.859 -0.859 -0.859]\n",
            "   ...\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   [-0.749 -0.749 -0.749]\n",
            "   [-0.757 -0.757 -0.757]]\n",
            "\n",
            "  [[-0.78  -0.78  -0.78 ]\n",
            "   [-0.788 -0.788 -0.788]\n",
            "   [-0.859 -0.859 -0.859]\n",
            "   ...\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   [-0.749 -0.749 -0.749]\n",
            "   [-0.765 -0.765 -0.765]]\n",
            "\n",
            "  [[-0.773 -0.773 -0.773]\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   ...\n",
            "   [-0.718 -0.718 -0.718]\n",
            "   [-0.741 -0.741 -0.741]\n",
            "   [-0.765 -0.765 -0.765]]]\n",
            "\n",
            "\n",
            " [[[-0.812 -0.812 -0.812]\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   ...\n",
            "   [-0.38  -0.38  -0.38 ]\n",
            "   [-0.349 -0.349 -0.349]\n",
            "   [-0.749 -0.749 -0.749]]\n",
            "\n",
            "  [[-0.804 -0.804 -0.804]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   ...\n",
            "   [-0.506 -0.506 -0.506]\n",
            "   [-0.42  -0.42  -0.42 ]\n",
            "   [-0.373 -0.373 -0.373]]\n",
            "\n",
            "  [[-0.812 -0.812 -0.812]\n",
            "   [-0.82  -0.82  -0.82 ]\n",
            "   [-0.843 -0.843 -0.843]\n",
            "   ...\n",
            "   [-0.608 -0.608 -0.608]\n",
            "   [ 0.035  0.035  0.035]\n",
            "   [ 0.122  0.122  0.122]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.773 -0.773 -0.773]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   ...\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.765 -0.765 -0.765]\n",
            "   [-0.765 -0.765 -0.765]]\n",
            "\n",
            "  [[-0.773 -0.773 -0.773]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   ...\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.765 -0.765 -0.765]\n",
            "   [-0.765 -0.765 -0.765]]\n",
            "\n",
            "  [[-0.749 -0.749 -0.749]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   ...\n",
            "   [-0.765 -0.765 -0.765]\n",
            "   [-0.765 -0.765 -0.765]\n",
            "   [-0.741 -0.741 -0.741]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[-0.882 -0.882 -0.882]\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   [-0.851 -0.851 -0.851]\n",
            "   ...\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.906 -0.906 -0.906]]\n",
            "\n",
            "  [[-0.702 -0.702 -0.702]\n",
            "   [-0.733 -0.733 -0.733]\n",
            "   [-0.788 -0.788 -0.788]\n",
            "   ...\n",
            "   [-0.663 -0.663 -0.663]\n",
            "   [-0.757 -0.757 -0.757]\n",
            "   [-0.898 -0.898 -0.898]]\n",
            "\n",
            "  [[-0.655 -0.655 -0.655]\n",
            "   [-0.663 -0.663 -0.663]\n",
            "   [-0.663 -0.663 -0.663]\n",
            "   ...\n",
            "   [-0.647 -0.647 -0.647]\n",
            "   [-0.671 -0.671 -0.671]\n",
            "   [-0.725 -0.725 -0.725]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.796 -0.796 -0.796]\n",
            "   [-0.765 -0.765 -0.765]\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   ...\n",
            "   [-0.725 -0.725 -0.725]\n",
            "   [-0.882 -0.882 -0.882]\n",
            "   [-0.843 -0.843 -0.843]]\n",
            "\n",
            "  [[-0.796 -0.796 -0.796]\n",
            "   [-0.765 -0.765 -0.765]\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   ...\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.882 -0.882 -0.882]\n",
            "   [-0.851 -0.851 -0.851]]\n",
            "\n",
            "  [[-0.804 -0.804 -0.804]\n",
            "   [-0.773 -0.773 -0.773]\n",
            "   [-0.812 -0.812 -0.812]\n",
            "   ...\n",
            "   [-0.639 -0.639 -0.639]\n",
            "   [-0.867 -0.867 -0.867]\n",
            "   [-0.859 -0.859 -0.859]]]\n",
            "\n",
            "\n",
            " [[[-0.961 -0.961 -0.961]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.906 -0.906 -0.906]\n",
            "   ...\n",
            "   [-0.906 -0.906 -0.906]\n",
            "   [-0.898 -0.898 -0.898]\n",
            "   [-0.898 -0.898 -0.898]]\n",
            "\n",
            "  [[-0.976 -0.976 -0.976]\n",
            "   [-0.937 -0.937 -0.937]\n",
            "   [-0.961 -0.961 -0.961]\n",
            "   ...\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.906 -0.906 -0.906]]\n",
            "\n",
            "  [[-0.984 -0.984 -0.984]\n",
            "   [-0.969 -0.969 -0.969]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   ...\n",
            "   [-0.945 -0.945 -0.945]\n",
            "   [-0.929 -0.929 -0.929]\n",
            "   [-0.922 -0.922 -0.922]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-0.976 -0.976 -0.976]]\n",
            "\n",
            "  [[-0.984 -0.984 -0.984]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-0.976 -0.976 -0.976]]\n",
            "\n",
            "  [[-0.984 -0.984 -0.984]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   ...\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-0.976 -0.976 -0.976]]]\n",
            "\n",
            "\n",
            " [[[-0.812 -0.812 -0.812]\n",
            "   [-0.914 -0.914 -0.914]\n",
            "   [-0.757 -0.757 -0.757]\n",
            "   ...\n",
            "   [-0.725 -0.725 -0.725]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   [-0.929 -0.929 -0.929]]\n",
            "\n",
            "  [[-0.82  -0.82  -0.82 ]\n",
            "   [-0.945 -0.945 -0.945]\n",
            "   [-0.804 -0.804 -0.804]\n",
            "   ...\n",
            "   [-0.686 -0.686 -0.686]\n",
            "   [-0.725 -0.725 -0.725]\n",
            "   [-0.827 -0.827 -0.827]]\n",
            "\n",
            "  [[-0.537 -0.537 -0.537]\n",
            "   [-0.788 -0.788 -0.788]\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   ...\n",
            "   [-0.671 -0.671 -0.671]\n",
            "   [-0.678 -0.678 -0.678]\n",
            "   [-0.765 -0.765 -0.765]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[-0.961 -0.961 -0.961]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.969 -0.969 -0.969]\n",
            "   ...\n",
            "   [-0.922 -0.922 -0.922]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.961 -0.961 -0.961]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.976 -0.976 -0.976]\n",
            "   ...\n",
            "   [-0.875 -0.875 -0.875]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-1.    -1.    -1.   ]]\n",
            "\n",
            "  [[-0.961 -0.961 -0.961]\n",
            "   [-1.    -1.    -1.   ]\n",
            "   [-0.984 -0.984 -0.984]\n",
            "   ...\n",
            "   [-0.796 -0.796 -0.796]\n",
            "   [-0.992 -0.992 -0.992]\n",
            "   [-1.    -1.    -1.   ]]]] [2 2 2 0 1 2 0 1 2 2 1 0 2 0 2 2 1 2 2 2 2 2 2 2 2 2 0 1 2 0 0 0 2 1 1 2 0\n",
            " 2 2 2 2 2 2 2 2 0 2 2 1 2 1 0 2 0 2 0 1 2 1 1 2 0 0 0 0 1 0 1 1 2 0 2 2 1\n",
            " 1 0 1 1 0 1 2 2 0 2 1 2 2 2 2 2 2 0 2 2 0 2 2 1 0 0]\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : NORMAL\n",
            "strLabel : VIRAL\n",
            "strLabel : BACTERIAL\n",
            "strLabel : NORMAL\n",
            "strLabel : VIRAL\n",
            "strLabel : BACTERIAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/xray/test_"
      ],
      "metadata": {
        "id": "TTkOd5VMrXOs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " pd.DataFrame(all_labels).value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7vW1UHftNcy",
        "outputId": "1f27ddee-c67c-47c0-8d43-8c5fc39dc7cf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    2530\n",
              "1    1345\n",
              "0    1325\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train model"
      ],
      "metadata": {
        "id": "EIFvMP0augMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a gan for generating faces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy import append\n",
        "from numpy.random import random\n",
        "from numpy.random import randint\n",
        "from numpy.random import shuffle\n",
        "import time\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from matplotlib import patheffects as path_effects\n",
        "import collections\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow import get_logger as log\n",
        "\n",
        "#  SET YOUR FLAGS\n",
        "qErrorHide = False\n",
        "if qErrorHide:\n",
        "    print(\"\\n***REMEMBER:  WARNINGS turned OFF***\\n***REMEMBER:  WARNINGS turned OFF***\\n\")\n",
        "    log().setLevel('ERROR')\n",
        "\n",
        "#    INDICATE IF STARTING FRESH OR CONTINUING FROM PREVIOUS RUN\n",
        "qRestart = False\n",
        "if qRestart:\n",
        "    epochs_done = 405\n",
        "    epochs_goal = 410\n",
        "else:\n",
        "    epochs_done = 0\n",
        "    epochs_goal = 100 \n",
        "\n",
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=(80,80,3), n_classes=3):\n",
        "\tprint(\"**********  ENTERED discriminator  *****************\")\n",
        "\t##### foundation for labels\n",
        "\tin_label = Input(shape=(1,))\n",
        "\tembedding_layer = Embedding(n_classes, 8)\n",
        "\t# embedding_layer.trainable = False\n",
        "\tli = embedding_layer (in_label)\n",
        "\tn_nodes = in_shape[0] * in_shape[1]\n",
        "\tprint(\">>embedding>> in_shape[0], in_shape[1], n_nodes: \", in_shape[0], in_shape[1], n_nodes)\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\tli = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
        "\t# image input\n",
        "\tdropout = 0.1\n",
        "\tin_image = Input(shape=in_shape)\n",
        "\tprint(\"\\nin_image: \", in_image)\n",
        "\t# concat label as a channel\n",
        "\tmerge = Concatenate()([in_image, li])\n",
        "\tprint(\"\\nmerge.shape: \", merge.shape)\n",
        "\t# sample to 80x80\n",
        "\tfe = Conv2D(128, (5,5), padding='same')(merge)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\tfe = Dropout(dropout)(fe)\n",
        "\tprint(\"fe.shape: \", fe.shape)\n",
        "\t# downsample to 40x40\n",
        "\tfe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# fe = Dropout(dropout)(fe)\n",
        "\tprint(\"fe.shape: \", fe.shape)\n",
        "\t# downsample to 20x20\n",
        "\tfe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# fe = Dropout(dropout)(fe)\n",
        "\tprint(\"fe.shape: \", fe.shape)\n",
        "\t# downsample to 10x10\n",
        "\tfe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# fe = Dropout(dropout)(fe)\n",
        "\tprint(\"fe.shape: \", fe.shape)\n",
        "\t# downsample to 5x5\n",
        "\tfe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
        "\tfe = LeakyReLU(alpha=0.2)(fe)\n",
        "\t# fe = Dropout(dropout)(fe)\n",
        "\tprint(\"fe.shape: \", fe.shape)\n",
        "\t# flatten feature maps\n",
        "\tfe = Flatten()(fe)\n",
        "\t# fe = Dropout(dropout)(fe)\n",
        "\tprint(\"fe flatten shape: \", fe.shape)\n",
        "\t# output\n",
        "\tout_layer = Dense(1, activation='sigmoid')(fe)\n",
        "\tprint(\"out_layer.shape: \", out_layer.shape)\n",
        "\t# define model\n",
        "\tmodel = Model([in_image, in_label], out_layer)\n",
        "\tprint(\"\\nmodel: \", model)\n",
        "\t# compile model\n",
        "\t# opt = Adamax(lr=0.00007, beta_1=0.08, beta_2=0.999, epsilon=10e-8)\n",
        "\topt = Adamax(lr=0.00004, beta_1=0.08, beta_2=0.999, epsilon=10e-8)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\tprint(\"\\nembedding_layer.get_weights(): \\n\",embedding_layer.get_weights())\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='xray/discriminator_model.png')\n",
        "\treturn model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(latent_dim, n_classes=3):\n",
        "\tprint(\"**********  ENTERED generator  *****************\")\n",
        "\t##### foundation for labels\n",
        "\tin_label = Input(shape=(1,))\n",
        "\tembedding_layer = Embedding(n_classes, 8)\n",
        "\tembedding_layer.trainable = True\n",
        "\tli = embedding_layer (in_label)\n",
        "\tn_nodes = 5 * 5\n",
        "\tli = Dense(n_nodes)(li)\n",
        "\tli = Reshape((5 , 5, 1))(li)\n",
        "\tprint(\"generator...  n_nodes, li.shape: \", n_nodes, li.shape)\n",
        "\t##### foundation for 5x5 image\n",
        "\tin_lat = Input(shape=(latent_dim,))\n",
        "\tn_nodes = 128 * 5 * 5\n",
        "\tgenX = Dense(n_nodes)(in_lat)\n",
        "\tgenX = LeakyReLU(alpha=0.2)(genX)\n",
        "\tgenX = Reshape((5, 5, 128))(genX)\n",
        "\tdropout = 0.1\n",
        "\tprint(\"genX.shape: \", genX.shape)\n",
        "\t##### merge image gen and label input\n",
        "\tmerge = Concatenate()([genX, li])\n",
        "\tprint(\"merge.shape: \", merge.shape)\n",
        "\t##### create merged model\n",
        "\t# upsample to 10x10\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
        "\tprint(\"gen after CV2DT.shape: \", gen.shape)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\tgen = Dropout(dropout)(gen)\n",
        "\tprint(\"gen.shape: \", gen.shape)\n",
        "\t# upsample to 20x20\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\tprint(\"gen.shape: \", gen.shape)\n",
        "\t# upsample to 40x40\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\tprint(\"gen.shape: \", gen.shape)\n",
        "\t# upsample to 80x80\n",
        "\tgen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
        "\tgen = LeakyReLU(alpha=0.2)(gen)\n",
        "\tprint(\"gen.shape: \", gen.shape)\n",
        "\t# output layer 80x80x3\n",
        "\tout_layer = Conv2D(3, (5,5), activation='tanh', padding='same')(gen)\n",
        "\tprint(\"out_layer.shape: \", out_layer.shape)\n",
        "\t# define model\n",
        "\tmodel = Model(inputs=[in_lat, in_label], outputs=out_layer)\n",
        "\topt = Adamax(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=10e-8)\n",
        "\tmodel.compile(loss=['binary_crossentropy'], optimizer=opt)\n",
        "\tprint(\"\\nembedding_layer.get_weights(): \\n\",embedding_layer.get_weights())\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='xray/generator_model.png')\n",
        "\treturn model\n",
        " \n",
        "# define the combined generator and discriminator model, for updating the generator\n",
        "def define_gan(g_model, d_model):\n",
        "\tprint(\"**********  ENTERED gan  *****************\")\n",
        "\t# make weights in the discriminator not trainable\n",
        "\td_model.trainable = False\n",
        "\t# get noise and label inputs from generator model\n",
        "\tgen_noise, gen_label = g_model.input\n",
        "\t# get image output from the generator model\n",
        "\tgen_output = g_model.output\n",
        "\t# connect image output and label input from generator as inputs to discriminator\n",
        "\tgan_output = d_model([gen_output, gen_label])\n",
        "\t# define gan model as taking noise and label and outputting a classification\n",
        "\tmodel = Model([gen_noise, gen_label], gan_output)\n",
        "\t# compile model\n",
        "\topt = Adamax(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=10e-8)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\tmodel.summary()\n",
        "\tplot_model(model, to_file='xray/gan_model.png')\n",
        "\treturn model\n",
        "\n",
        "# assign categories\n",
        "def assign_categs(df, lenrows):\n",
        "\tprint(\"\\n*****  ATTRIBUTES: \\n\", df.mean())\n",
        "\treturn categs\n",
        "\n",
        "def get_cumProbs(freqCategs, categs):\n",
        "\tfreqLists = [freqCategs[i][1] for i in range(len(freqCategs))]\n",
        "\tfreqListX = asarray(freqLists, dtype=np.float32)\n",
        "\tprint(\"freqListX: \", freqListX)\n",
        "\tprint(\"len(categs): \", len(categs))\n",
        "\tcumProbs = freqListX/len(categs)\n",
        "\tprint(\"cumProbs: \", cumProbs)\n",
        "\tcumProbs = append((0.0),cumProbs)\n",
        "\tfor i in range(len(cumProbs)-1):\n",
        "\t\tcumProbs[i+1]=cumProbs[i]+cumProbs[i+1]\n",
        "\tprint(\"cumProbs: \", cumProbs)\n",
        "\treturn cumProbs\n",
        "\n",
        "\n",
        " \n",
        "# load and prepare training images\n",
        "def load_real_samples():\n",
        "\t# load the face dataset\n",
        "\tdata = load('xray/img_align_xray.npz')\n",
        "\tX = data['arr_0']\n",
        "\t# convert from unsigned ints to floats\n",
        "\tX = X.astype('float32')\n",
        "\t# scale from [0,255] to [-1,1]\n",
        "\tX = (X - 127.5) / 127.5\n",
        "\tdata  = load('xray/labels_align_xray.npz')\n",
        "\tlabels = data['arr_0']\n",
        "\tprint(\"labels: \", labels)\n",
        "\tlenLabels = len(labels)\n",
        "\tprint(\"lenLabels: \", lenLabels)\n",
        "\tlenrows = len(X)\n",
        "\tfreqCategs = list(collections.Counter(sorted(labels)).items())\n",
        "\tprint(\"freqCategs: \", freqCategs)\n",
        "\tcumProbs = get_cumProbs(freqCategs, labels)\n",
        "\treturn [X, labels], cumProbs\n",
        " \n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# print(\"n_samples: \", n_samples)\n",
        "\t# split into images and labels\n",
        "\timages, labels = dataset\n",
        "\t# choose random instances\n",
        "\tix = randint(0, images.shape[0], n_samples)\n",
        "\t# print(\"ix: \", ix)\n",
        "\t# print(\"images.size: \", images.size)\n",
        "\t# print(\"labels.size: \", labels.size)\n",
        "\t# retrieve selected images\n",
        "\tX, labels = images[ix], labels[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn [X, labels], y\n",
        " \n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, cumProbs, n_classes=3):\n",
        "\t# print(\"generate_latent_points: \", latent_dim, n_samples)\n",
        "\tinitX = -3.0\n",
        "\trangeX = 2.0*abs(initX)\n",
        "\tstepX = rangeX / (latent_dim * n_samples)\n",
        "\tx_input = asarray([initX + stepX*(float(i)) for i in range(0,latent_dim * n_samples)])\n",
        "\tshuffle(x_input)\n",
        "\t# generate points in the latent space\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\trandx = random(n_samples)\n",
        "\tlabels = np.zeros(n_samples, dtype=int)\n",
        "\tfor i in range(n_classes):\n",
        "\t\tlabels = np.where((randx >= cumProbs[i]) & (randx < cumProbs[i+1]), i, labels)\n",
        "\treturn [z_input, labels]\n",
        " \n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples, cumProbs):\n",
        "\t# generate points in latent space\n",
        "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples, cumProbs)\n",
        "\t# predict outputs\n",
        "\timages = generator.predict([z_input, labels_input])\n",
        "\t# create class labels\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn [images, labels_input], y\n",
        " \n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, labels, epoch, n=10):\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\texamples = (examples + 1) / 2.0\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tfig = plt.subplot(n, n, 1 + i)\n",
        "\t\tstrLabel = str(labels[i])\n",
        "\t\t# turn off axis\n",
        "\t\tfig.axis('off')\n",
        "\t\tfig.text(8.0,20.0,strLabel, fontsize=6, color='white')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tfig.imshow(examples[i])\n",
        "\t# save plot to file\n",
        "\tfilename = 'xray/results/generated_plot_e%03d.png' % (epoch+1)\n",
        "\tplt.savefig(filename)\n",
        "\tplt.close()\n",
        "\t\n",
        "def save_real_plots(dataset, nRealPlots = 5, n=10, n_samples=100):\n",
        "\t# plot images\n",
        "\tfor epoch in range(nRealPlots):\n",
        "\t\tif epoch%5==0:\n",
        "\t\t\tprint(\"real_plots: \", epoch)\n",
        "\t\t# prepare real samples\n",
        "\t\t[X_real, labels], y_real = generate_real_samples(dataset, n_samples)\n",
        "\t\t# scale from [-1,1] to [0,1]\n",
        "\t\tX_real = (X_real + 1) / 2.0\n",
        "\t\tfor i in range(n * n):\n",
        "\t\t\t# define subplot\n",
        "\t\t\tfig = plt.subplot(n, n, 1 + i)\n",
        "\t\t\tstrLabel = str(labels[i])\n",
        "\t\t\t# fig.title = strLabel\n",
        "\t\t\t# turn off axis\n",
        "\t\t\tfig.axis('off')\n",
        "\t\t\tfig.text(8.0,20.0,strLabel, fontsize=6, color='white')\n",
        "\t\t\t# plot raw pixel data\n",
        "\t\t\tfig.imshow(X_real[i])\n",
        "\t\t# save plot to file\n",
        "\t\tfilename = 'xray/real_plots/real_plot_e%03d.png' % (epoch+1)\n",
        "\t\tplt.savefig(filename)\n",
        "\t\tplt.close()\n",
        " \n",
        "# evaluate the discriminator, plot generated images, save generator model\n",
        "def summarize_performance(epoch, g_model, d_model, gan_model, dataset, latent_dim, n_samples=100):\n",
        "    # prepare real samples\n",
        "    [X_real, labels_real], y_real = generate_real_samples(dataset, n_samples)\n",
        "    # evaluate discriminator on real examples\n",
        "    _, acc_real = d_model.evaluate([X_real, labels_real], y_real, verbose=0)\n",
        "    # prepare fake examples\n",
        "    [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, n_samples, cumProbs)\n",
        "    # evaluate discriminator on fake examples\n",
        "    _, acc_fake = d_model.evaluate([X_fake, labels], y_fake, verbose=0)\n",
        "    # summarize discriminator performance\n",
        "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "    # save plot\n",
        "    save_plot(X_fake, labels, epoch)\n",
        "    # save the generator model tile file\n",
        "    filename = 'xray/results/generator_model_%03d.h5' % (epoch+1)\n",
        "    g_model.save(filename)\n",
        "    #save on drive\n",
        "    filename_generate = '/content/drive/MyDrive/cGAN_xray/xray/results/generator_model_%03d.h5' % (epoch+1)\n",
        "    g_model.save(filename_generate)\n",
        " \n",
        "    filename = 'xray/results/generator_model_gan%03d.h5' % (epoch+1)\n",
        "    gan_model.save(filename)\n",
        " \n",
        "    #save on drive\n",
        "    filename_gan = '/content/drive/MyDrive/cGAN_xray/xray/results/generator_model_gan%03d.h5' % (epoch+1)\n",
        "    gan_model.save(filename_gan)\n",
        " \n",
        "    filename = 'xray/results/generator_model_dis%03d.h5' % (epoch+1)\n",
        "    filename_dis = '/content/drive/MyDrive/cGAN_xray/xray/results/generator_model_dis%03d.h5' % (epoch+1)\n",
        "\n",
        "    d_model.trainable = True\n",
        "    for layer in d_model.layers:\n",
        "        layer.trainable = True\n",
        "    d_model.save(filename)\n",
        "    d_model.save(filename_dis)\n",
        "    d_model.trainable = False\n",
        "    for layer in d_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "\n",
        "def restart(epochs_done):\n",
        "\t# gen_weights = array(model.get_weights())\n",
        "\tprint(\"****  restart PULLING IN EPOCH: \", epochs_done)\n",
        "\tfilename = 'xray/results/generator_model_dis%03d.h5' % (epochs_done)\n",
        "\td_model = load_model(filename, compile=True)\n",
        "\td_model.trainable = True\n",
        "\tfor layer in d_model.layers:\n",
        "\t\tlayer.trainable = True\n",
        "\td_model.summary()\n",
        "\tfilename = 'xray/results/generator_model_%03d.h5' % (epochs_done)\n",
        "\tg_model = load_model(filename, compile=True)\n",
        "\tg_model.summary()\n",
        "\tgan_model = define_gan(g_model, d_model)\n",
        "\tgan_model.summary()\n",
        "\treturn d_model, g_model, gan_model\n",
        "\n",
        " \n",
        "# train the generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, epochs_goal=100, n_batch=128, epochs_done=0):\n",
        "\tnTryAgains = 0\n",
        "\tnTripsOnSameSavedWts = 0\n",
        "\tnSaves = 0\n",
        "\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)\n",
        "\td_trainable_weights = np.array(d_model.get_weights())\n",
        "\tg_trainable_weights = np.array(g_model.get_weights())\n",
        "\tgan_trainable_weights = np.array(gan_model.get_weights())\n",
        "\tnow = time.time()\n",
        "\tij = 0\n",
        "\tijSave = -100\n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(epochs_done, epochs_goal):\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\tij+=1\n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "\t\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "\t\t\tqDebug=False\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\tdis_loss, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "\t\t\t[X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch, cumProbs)\n",
        "\t\t\tgen_loss, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "\t\t\t[z_input, labels_input] = generate_latent_points(latent_dim, n_batch, cumProbs)\n",
        "\t\t\t# create inverted labels for the fake samples\n",
        "\t\t\ty_gan = ones((n_batch, 1))\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tgan_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "\t\t\t# summarize loss on this batch\n",
        "\t\t\tif (j+1) % 5==0 or dis_loss > 1.10 or gen_loss > 1.10 or gan_loss > 1.80:\n",
        "\t\t\t\tdiff = int(time.time()-now)\n",
        "\t\t\t\tprint('>%d/%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f, secs=%d, tryAgain=%d, nTripsOnSameSavedWts=%d, nSaves=%d' %\n",
        "\t\t\t\t\t(i+1, epochs_goal, j+1, bat_per_epo, dis_loss, gen_loss, gan_loss, diff, nTryAgains, nTripsOnSameSavedWts, nSaves))\n",
        "\t\t\tif dis_loss > 0.30 and dis_loss < 0.95 and gen_loss > 0.25 and gen_loss < 0.95 and gan_loss > 0.40 and gan_loss < 1.50:\n",
        "\t\t\t\tnTripsOnSameSavedWts = 0\n",
        "\t\t\t\tif ij - ijSave > 8:\n",
        "\t\t\t\t\tnSaves+=1\n",
        "\t\t\t\t\tijSave = ij\n",
        "\t\t\t\t\td_trainable_weights = np.array(d_model.get_weights())\n",
        "\t\t\t\t\tg_trainable_weights = np.array(g_model.get_weights())\n",
        "\t\t\t\t\tgan_trainable_weights = np.array(gan_model.get_weights())\n",
        "\t\t\tif (dis_loss < 0.001 or dis_loss > 2.0) and ijSave > 0:\n",
        "\t\t\t\tnTryAgains+=1\n",
        "\t\t\t\tnTripsOnSameSavedWts+=1\n",
        "\t\t\t\tprint(\"LOADING d_model\",j+1,\" from \",ijSave)\n",
        "\t\t\t\td_model.set_weights(d_trainable_weights)\n",
        "\t\t\tif (gen_loss < 0.001 or gen_loss > 2.0) and ijSave > 0:\n",
        "\t\t\t\tnTryAgains+=1\n",
        "\t\t\t\tnTripsOnSameSavedWts+=1\n",
        "\t\t\t\tprint(\"LOADING g_model\",j+1,\" from \",ijSave)\n",
        "\t\t\t\tg_model.set_weights(g_trainable_weights)\n",
        "\t\t\tif (gan_loss < 0.010 or gan_loss > 3.00) and ijSave > 0:\n",
        "\t\t\t\tnTryAgains+=1\n",
        "\t\t\t\tnTripsOnSameSavedWts+=1\n",
        "\t\t\t\tprint(\"LOADING gan_models\",j+1,\" from \",ijSave)\n",
        "\t\t\t\tgan_model.set_weights(gan_trainable_weights)\n",
        "\t\t\t# if (j+1) % 10 == 0:\n",
        "\t\t\t\t# summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "\t\t\tif nTripsOnSameSavedWts > 20:\n",
        "\t\t\t\tprint(\"**********  Too many rebuilds  **************\")\n",
        "\t\t\t\tsummarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "\t\t\t\timport sys\n",
        "\t\t\t\tsys.exit(0)\n",
        "\t\t# evaluate the model performance, sometimes\n",
        "\t\tif (i+1) % 10 == 0:\n",
        "\t\t\tsummarize_performance(i, g_model, d_model, gan_model, dataset, latent_dim)\n",
        "\n",
        "# size of the latent space\n",
        "latent_dim = 100\n",
        "\n",
        "if qRestart:\n",
        "        d_model, g_model, gan_model = restart(epochs_done = epochs_done)\n",
        "else:\n",
        "        # create the discriminator\n",
        "        d_model = define_discriminator()\n",
        "        # create the generator\n",
        "        g_model = define_generator(latent_dim)\n",
        "        # create the gan\n",
        "        gan_model = define_gan(g_model, d_model)\n",
        "        \n",
        "# load image data\n",
        "dataset, cumProbs = load_real_samples()\n",
        "save_real_plots(dataset, nRealPlots=2)\n",
        "train(g_model, d_model, gan_model,  dataset, latent_dim, epochs_goal=epochs_goal, n_batch=64, epochs_done=epochs_done)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy6jPIeiuuAQ",
        "outputId": "3d69b319-aff6-4c4c-c6e4-68bd961deff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********  ENTERED discriminator  *****************\n",
            ">>embedding>> in_shape[0], in_shape[1], n_nodes:  80 80 6400\n",
            "\n",
            "in_image:  KerasTensor(type_spec=TensorSpec(shape=(None, 80, 80, 3), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\")\n",
            "\n",
            "merge.shape:  (None, 80, 80, 4)\n",
            "fe.shape:  (None, 80, 80, 128)\n",
            "fe.shape:  (None, 40, 40, 128)\n",
            "fe.shape:  (None, 20, 20, 128)\n",
            "fe.shape:  (None, 10, 10, 128)\n",
            "fe.shape:  (None, 5, 5, 128)\n",
            "fe flatten shape:  (None, 3200)\n",
            "out_layer.shape:  (None, 1)\n",
            "\n",
            "model:  <keras.engine.functional.Functional object at 0x7f0dba603790>\n",
            "\n",
            "embedding_layer.get_weights(): \n",
            " [array([[-0.03073829, -0.01215079, -0.00531125, -0.02493281,  0.01293297,\n",
            "         0.04408422, -0.04566335, -0.04072207],\n",
            "       [ 0.00952963, -0.02093607,  0.04980237, -0.02807131,  0.03324424,\n",
            "        -0.02106849, -0.01918643, -0.04431773],\n",
            "       [ 0.02181245,  0.01519055,  0.01509266,  0.00776792, -0.02830525,\n",
            "         0.04420697, -0.02943773,  0.01485443]], dtype=float32)]\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 1, 8)         24          ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1, 6400)      57600       ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 80, 80, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 80, 80, 1)    0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 80, 80, 4)    0           ['input_2[0][0]',                \n",
            "                                                                  'reshape[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 80, 80, 128)  12928       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)        (None, 80, 80, 128)  0           ['conv2d[0][0]']                 \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 80, 80, 128)  0           ['leaky_re_lu[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 40, 40, 128)  409728      ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)      (None, 40, 40, 128)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 20, 20, 128)  409728      ['leaky_re_lu_1[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)      (None, 20, 20, 128)  0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 10, 10, 128)  409728      ['leaky_re_lu_2[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)      (None, 10, 10, 128)  0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 5, 5, 128)    409728      ['leaky_re_lu_3[0][0]']          \n",
            "                                                                                                  \n",
            " leaky_re_lu_4 (LeakyReLU)      (None, 5, 5, 128)    0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 3200)         0           ['leaky_re_lu_4[0][0]']          \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            3201        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,712,665\n",
            "Trainable params: 1,712,665\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adamax.py:90: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adamax, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********  ENTERED generator  *****************\n",
            "generator...  n_nodes, li.shape:  25 (None, 5, 5, 1)\n",
            "genX.shape:  (None, 5, 5, 128)\n",
            "merge.shape:  (None, 5, 5, 129)\n",
            "gen after CV2DT.shape:  (None, 10, 10, 128)\n",
            "gen.shape:  (None, 10, 10, 128)\n",
            "gen.shape:  (None, 20, 20, 128)\n",
            "gen.shape:  (None, 40, 40, 128)\n",
            "gen.shape:  (None, 80, 80, 128)\n",
            "out_layer.shape:  (None, 80, 80, 3)\n",
            "\n",
            "embedding_layer.get_weights(): \n",
            " [array([[ 0.02609141,  0.04059908,  0.03537095, -0.03931664,  0.00807967,\n",
            "         0.00598494, -0.01399331, -0.02770642],\n",
            "       [ 0.04869303, -0.03044518,  0.01213712,  0.00357984,  0.04108752,\n",
            "         0.03712373,  0.03362192, -0.03308418],\n",
            "       [ 0.03968811,  0.00387245,  0.01346829,  0.04375852,  0.0277721 ,\n",
            "        -0.04825039,  0.03543046,  0.03246847]], dtype=float32)]\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 3200)         323200      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 1, 8)         24          ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " leaky_re_lu_5 (LeakyReLU)      (None, 3200)         0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1, 25)        225         ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 5, 5, 128)    0           ['leaky_re_lu_5[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 5, 5, 1)      0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 5, 5, 129)    0           ['reshape_2[0][0]',              \n",
            "                                                                  'reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 10, 10, 128)  264320     ['concatenate_1[0][0]']          \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " leaky_re_lu_6 (LeakyReLU)      (None, 10, 10, 128)  0           ['conv2d_transpose[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 10, 10, 128)  0           ['leaky_re_lu_6[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 20, 20, 128)  262272     ['dropout_1[0][0]']              \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " leaky_re_lu_7 (LeakyReLU)      (None, 20, 20, 128)  0           ['conv2d_transpose_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 40, 40, 128)  262272     ['leaky_re_lu_7[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " leaky_re_lu_8 (LeakyReLU)      (None, 40, 40, 128)  0           ['conv2d_transpose_2[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 80, 80, 128)  262272     ['leaky_re_lu_8[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " leaky_re_lu_9 (LeakyReLU)      (None, 80, 80, 128)  0           ['conv2d_transpose_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 80, 80, 3)    9603        ['leaky_re_lu_9[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,384,188\n",
            "Trainable params: 1,384,188\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "**********  ENTERED gan  *****************\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 3200)         323200      ['input_4[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 1, 8)         24          ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " leaky_re_lu_5 (LeakyReLU)      (None, 3200)         0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1, 25)        225         ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 5, 5, 128)    0           ['leaky_re_lu_5[0][0]']          \n",
            "                                                                                                  \n",
            " reshape_1 (Reshape)            (None, 5, 5, 1)      0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 5, 5, 129)    0           ['reshape_2[0][0]',              \n",
            "                                                                  'reshape_1[0][0]']              \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 10, 10, 128)  264320     ['concatenate_1[0][0]']          \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " leaky_re_lu_6 (LeakyReLU)      (None, 10, 10, 128)  0           ['conv2d_transpose[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 10, 10, 128)  0           ['leaky_re_lu_6[0][0]']          \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 20, 20, 128)  262272     ['dropout_1[0][0]']              \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " leaky_re_lu_7 (LeakyReLU)      (None, 20, 20, 128)  0           ['conv2d_transpose_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 40, 40, 128)  262272     ['leaky_re_lu_7[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " leaky_re_lu_8 (LeakyReLU)      (None, 40, 40, 128)  0           ['conv2d_transpose_2[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 80, 80, 128)  262272     ['leaky_re_lu_8[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " leaky_re_lu_9 (LeakyReLU)      (None, 80, 80, 128)  0           ['conv2d_transpose_3[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 80, 80, 3)    9603        ['leaky_re_lu_9[0][0]']          \n",
            "                                                                                                  \n",
            " model (Functional)             (None, 1)            1712665     ['conv2d_5[0][0]',               \n",
            "                                                                  'input_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3,096,853\n",
            "Trainable params: 1,384,188\n",
            "Non-trainable params: 1,712,665\n",
            "__________________________________________________________________________________________________\n",
            "labels:  [2 1 1 ... 0 0 0]\n",
            "lenLabels:  5200\n",
            "freqCategs:  [(0, 1325), (1, 1345), (2, 2530)]\n",
            "freqListX:  [1325. 1345. 2530.]\n",
            "len(categs):  5200\n",
            "cumProbs:  [0.25480768 0.25865385 0.48653847]\n",
            "cumProbs:  [0.         0.25480768 0.51346153 1.        ]\n",
            "real_plots:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:377: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:378: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:379: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:410: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:411: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:412: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1/100, 5/81, d1=0.578, d2=0.699, g=0.688, secs=9, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=1\n",
            ">1/100, 10/81, d1=0.391, d2=0.745, g=0.652, secs=11, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=1\n",
            ">1/100, 15/81, d1=0.206, d2=0.815, g=0.645, secs=13, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 20/81, d1=0.108, d2=0.560, g=0.909, secs=15, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 25/81, d1=0.045, d2=0.628, g=0.771, secs=17, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 30/81, d1=0.026, d2=0.689, g=0.702, secs=19, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 35/81, d1=0.061, d2=1.033, g=0.478, secs=21, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 36/81, d1=0.055, d2=1.128, g=0.436, secs=21, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 37/81, d1=0.075, d2=1.230, g=0.404, secs=22, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 38/81, d1=0.066, d2=1.328, g=0.379, secs=22, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 39/81, d1=0.070, d2=1.385, g=0.371, secs=23, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 40/81, d1=0.051, d2=1.391, g=0.389, secs=23, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 41/81, d1=0.085, d2=1.302, g=0.442, secs=23, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 42/81, d1=0.130, d2=1.150, g=0.545, secs=24, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 45/81, d1=0.090, d2=0.502, g=1.250, secs=25, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 50/81, d1=0.072, d2=0.287, g=1.507, secs=27, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 55/81, d1=0.062, d2=0.542, g=0.889, secs=29, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 60/81, d1=0.035, d2=0.667, g=0.724, secs=31, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 65/81, d1=0.025, d2=0.690, g=0.698, secs=33, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 70/81, d1=0.014, d2=0.689, g=0.699, secs=35, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 75/81, d1=0.020, d2=0.690, g=0.698, secs=37, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">1/100, 80/81, d1=0.017, d2=0.689, g=0.699, secs=39, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 5/81, d1=0.026, d2=0.692, g=0.697, secs=42, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 10/81, d1=0.035, d2=0.700, g=0.691, secs=44, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 15/81, d1=0.026, d2=0.710, g=0.679, secs=46, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 20/81, d1=0.013, d2=0.727, g=0.667, secs=48, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 25/81, d1=0.028, d2=0.746, g=0.647, secs=50, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 30/81, d1=0.019, d2=0.771, g=0.627, secs=52, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 35/81, d1=0.025, d2=0.803, g=0.620, secs=54, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 40/81, d1=0.022, d2=0.796, g=0.632, secs=56, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 45/81, d1=0.085, d2=0.745, g=0.669, secs=58, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 50/81, d1=0.068, d2=0.692, g=0.736, secs=60, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 55/81, d1=0.084, d2=0.643, g=0.780, secs=62, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 60/81, d1=0.091, d2=0.671, g=0.836, secs=64, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 65/81, d1=0.091, d2=0.671, g=0.822, secs=66, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 70/81, d1=0.249, d2=0.684, g=0.728, secs=68, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 75/81, d1=0.152, d2=0.741, g=0.730, secs=70, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">2/100, 80/81, d1=0.197, d2=0.811, g=0.693, secs=72, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">3/100, 5/81, d1=0.244, d2=0.894, g=0.651, secs=75, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">3/100, 10/81, d1=0.370, d2=0.913, g=0.655, secs=77, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=2\n",
            ">3/100, 15/81, d1=0.327, d2=0.936, g=0.624, secs=79, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=3\n",
            ">3/100, 20/81, d1=0.305, d2=0.970, g=0.661, secs=81, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=4\n",
            ">3/100, 25/81, d1=0.393, d2=0.915, g=0.677, secs=83, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=4\n",
            ">3/100, 30/81, d1=0.385, d2=0.847, g=0.695, secs=85, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=5\n",
            ">3/100, 35/81, d1=0.306, d2=0.907, g=0.765, secs=87, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=5\n",
            ">3/100, 40/81, d1=0.345, d2=0.855, g=0.745, secs=89, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=6\n",
            ">3/100, 45/81, d1=0.342, d2=0.876, g=0.661, secs=91, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=6\n",
            ">3/100, 50/81, d1=0.372, d2=0.929, g=0.630, secs=93, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=7\n",
            ">3/100, 55/81, d1=0.407, d2=0.931, g=0.626, secs=95, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=7\n",
            ">3/100, 60/81, d1=0.455, d2=0.971, g=0.639, secs=97, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=8\n",
            ">3/100, 65/81, d1=0.451, d2=0.914, g=0.660, secs=99, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=9\n",
            ">3/100, 70/81, d1=0.456, d2=0.900, g=0.639, secs=101, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=9\n",
            ">3/100, 75/81, d1=0.492, d2=0.867, g=0.675, secs=104, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=10\n",
            ">3/100, 80/81, d1=0.465, d2=0.843, g=0.692, secs=106, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=10\n",
            ">4/100, 5/81, d1=0.477, d2=0.787, g=0.743, secs=108, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=11\n",
            ">4/100, 10/81, d1=0.559, d2=0.799, g=0.799, secs=110, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=11\n",
            ">4/100, 15/81, d1=0.499, d2=0.774, g=0.779, secs=112, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=12\n",
            ">4/100, 20/81, d1=0.545, d2=0.756, g=0.838, secs=114, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=13\n",
            ">4/100, 25/81, d1=0.622, d2=0.772, g=0.823, secs=116, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=13\n",
            ">4/100, 30/81, d1=0.611, d2=0.768, g=0.813, secs=119, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=14\n",
            ">4/100, 35/81, d1=0.616, d2=0.751, g=0.810, secs=121, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=14\n",
            ">4/100, 40/81, d1=0.626, d2=0.722, g=0.826, secs=123, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=15\n",
            ">4/100, 45/81, d1=0.630, d2=0.726, g=0.848, secs=125, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=15\n",
            ">4/100, 50/81, d1=0.680, d2=0.764, g=0.805, secs=127, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=16\n",
            ">4/100, 55/81, d1=0.618, d2=0.663, g=0.841, secs=129, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=16\n",
            ">4/100, 60/81, d1=0.655, d2=0.702, g=0.813, secs=131, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=17\n",
            ">4/100, 65/81, d1=0.631, d2=0.744, g=0.771, secs=133, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=18\n",
            ">4/100, 70/81, d1=0.556, d2=0.730, g=0.750, secs=135, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=18\n",
            ">4/100, 75/81, d1=0.555, d2=0.773, g=0.721, secs=137, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=19\n",
            ">4/100, 80/81, d1=0.604, d2=0.769, g=0.763, secs=140, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=19\n",
            ">5/100, 5/81, d1=0.504, d2=0.747, g=0.760, secs=142, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=20\n",
            ">5/100, 10/81, d1=0.490, d2=0.780, g=0.747, secs=144, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=20\n",
            ">5/100, 15/81, d1=0.481, d2=0.734, g=0.745, secs=146, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=21\n",
            ">5/100, 20/81, d1=0.536, d2=0.775, g=0.760, secs=148, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=22\n",
            ">5/100, 25/81, d1=0.579, d2=0.748, g=0.742, secs=150, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=22\n",
            ">5/100, 30/81, d1=0.534, d2=0.748, g=0.771, secs=153, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=23\n",
            ">5/100, 35/81, d1=0.595, d2=0.759, g=0.779, secs=155, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=23\n",
            ">5/100, 40/81, d1=0.620, d2=0.727, g=0.773, secs=157, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=24\n",
            ">5/100, 45/81, d1=0.562, d2=0.762, g=0.752, secs=159, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=24\n",
            ">5/100, 50/81, d1=0.571, d2=0.812, g=0.713, secs=161, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=25\n",
            ">5/100, 55/81, d1=0.643, d2=0.733, g=0.752, secs=163, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=25\n",
            ">5/100, 60/81, d1=0.671, d2=0.832, g=0.668, secs=165, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=26\n",
            ">5/100, 65/81, d1=0.573, d2=0.768, g=0.717, secs=167, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=27\n",
            ">5/100, 70/81, d1=0.594, d2=0.782, g=0.762, secs=169, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=27\n",
            ">5/100, 75/81, d1=0.597, d2=0.722, g=0.777, secs=171, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=28\n",
            ">5/100, 80/81, d1=0.610, d2=0.750, g=0.789, secs=174, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=28\n",
            ">6/100, 5/81, d1=0.678, d2=0.689, g=0.770, secs=176, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=29\n",
            ">6/100, 10/81, d1=0.640, d2=0.726, g=0.777, secs=178, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=29\n",
            ">6/100, 15/81, d1=0.700, d2=0.698, g=0.792, secs=180, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=30\n",
            ">6/100, 20/81, d1=0.644, d2=0.680, g=0.802, secs=182, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=31\n",
            ">6/100, 25/81, d1=0.629, d2=0.691, g=0.786, secs=184, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=31\n",
            ">6/100, 30/81, d1=0.571, d2=0.679, g=0.783, secs=186, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=32\n",
            ">6/100, 35/81, d1=0.566, d2=0.692, g=0.773, secs=189, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=32\n",
            ">6/100, 40/81, d1=0.569, d2=0.754, g=0.697, secs=191, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=33\n",
            ">6/100, 45/81, d1=0.578, d2=0.837, g=0.635, secs=193, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=33\n",
            ">6/100, 50/81, d1=0.552, d2=0.808, g=0.671, secs=195, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=34\n",
            ">6/100, 55/81, d1=0.591, d2=0.755, g=0.726, secs=197, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=34\n",
            ">6/100, 60/81, d1=0.563, d2=0.718, g=0.779, secs=199, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=35\n",
            ">6/100, 65/81, d1=0.572, d2=0.696, g=0.771, secs=201, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=36\n",
            ">6/100, 70/81, d1=0.531, d2=0.722, g=0.792, secs=203, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=36\n",
            ">6/100, 75/81, d1=0.593, d2=0.765, g=0.731, secs=205, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=37\n",
            ">6/100, 80/81, d1=0.629, d2=0.780, g=0.732, secs=207, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=37\n",
            ">7/100, 5/81, d1=0.641, d2=0.772, g=0.750, secs=210, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=38\n",
            ">7/100, 10/81, d1=0.603, d2=0.744, g=0.740, secs=212, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=38\n",
            ">7/100, 15/81, d1=0.619, d2=0.747, g=0.764, secs=214, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=39\n",
            ">7/100, 20/81, d1=0.620, d2=0.745, g=0.740, secs=216, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=40\n",
            ">7/100, 25/81, d1=0.611, d2=0.754, g=0.746, secs=218, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=40\n",
            ">7/100, 30/81, d1=0.600, d2=0.770, g=0.688, secs=220, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=41\n",
            ">7/100, 35/81, d1=0.638, d2=0.760, g=0.736, secs=222, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=41\n",
            ">7/100, 40/81, d1=0.571, d2=0.777, g=0.696, secs=225, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=42\n",
            ">7/100, 45/81, d1=0.631, d2=0.762, g=0.742, secs=227, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=42\n",
            ">7/100, 50/81, d1=0.602, d2=0.755, g=0.731, secs=229, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=43\n",
            ">7/100, 55/81, d1=0.636, d2=0.713, g=0.772, secs=231, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=43\n",
            ">7/100, 60/81, d1=0.601, d2=0.696, g=0.752, secs=233, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=44\n",
            ">7/100, 65/81, d1=0.618, d2=0.713, g=0.785, secs=235, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=45\n",
            ">7/100, 70/81, d1=0.653, d2=0.738, g=0.742, secs=237, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=45\n",
            ">7/100, 75/81, d1=0.657, d2=0.747, g=0.725, secs=239, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=46\n",
            ">7/100, 80/81, d1=0.643, d2=0.748, g=0.723, secs=241, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=46\n",
            ">8/100, 5/81, d1=0.639, d2=0.702, g=0.745, secs=244, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=47\n",
            ">8/100, 10/81, d1=0.634, d2=0.713, g=0.752, secs=246, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=47\n",
            ">8/100, 15/81, d1=0.648, d2=0.716, g=0.756, secs=248, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=48\n",
            ">8/100, 20/81, d1=0.626, d2=0.737, g=0.717, secs=250, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=49\n",
            ">8/100, 25/81, d1=0.627, d2=0.743, g=0.702, secs=252, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=49\n",
            ">8/100, 30/81, d1=0.611, d2=0.749, g=0.701, secs=254, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=50\n",
            ">8/100, 35/81, d1=0.594, d2=0.756, g=0.692, secs=256, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=50\n",
            ">8/100, 40/81, d1=0.606, d2=0.762, g=0.693, secs=258, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=51\n",
            ">8/100, 45/81, d1=0.580, d2=0.743, g=0.703, secs=260, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=51\n",
            ">8/100, 50/81, d1=0.600, d2=0.734, g=0.719, secs=263, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=52\n",
            ">8/100, 55/81, d1=0.591, d2=0.724, g=0.734, secs=265, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=52\n",
            ">8/100, 60/81, d1=0.605, d2=0.719, g=0.743, secs=267, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=53\n",
            ">8/100, 65/81, d1=0.606, d2=0.730, g=0.734, secs=269, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=54\n",
            ">8/100, 70/81, d1=0.625, d2=0.754, g=0.735, secs=271, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=54\n",
            ">8/100, 75/81, d1=0.647, d2=0.708, g=0.749, secs=273, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=55\n",
            ">8/100, 80/81, d1=0.631, d2=0.696, g=0.759, secs=275, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=55\n",
            ">9/100, 5/81, d1=0.630, d2=0.691, g=0.771, secs=278, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=56\n",
            ">9/100, 10/81, d1=0.636, d2=0.706, g=0.754, secs=280, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=56\n",
            ">9/100, 15/81, d1=0.616, d2=0.742, g=0.730, secs=282, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=57\n",
            ">9/100, 20/81, d1=0.590, d2=0.767, g=0.712, secs=284, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=58\n",
            ">9/100, 25/81, d1=0.587, d2=0.743, g=0.726, secs=286, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=58\n",
            ">9/100, 30/81, d1=0.597, d2=0.744, g=0.713, secs=288, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=59\n",
            ">9/100, 35/81, d1=0.604, d2=0.756, g=0.720, secs=290, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=59\n",
            ">9/100, 40/81, d1=0.585, d2=0.738, g=0.737, secs=292, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=60\n",
            ">9/100, 45/81, d1=0.582, d2=0.745, g=0.746, secs=294, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=60\n",
            ">9/100, 50/81, d1=0.608, d2=0.741, g=0.733, secs=297, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=61\n",
            ">9/100, 55/81, d1=0.629, d2=0.720, g=0.732, secs=299, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=61\n",
            ">9/100, 60/81, d1=0.662, d2=0.718, g=0.739, secs=301, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=62\n",
            ">9/100, 65/81, d1=0.645, d2=0.708, g=0.743, secs=303, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=63\n",
            ">9/100, 70/81, d1=0.685, d2=0.685, g=0.762, secs=305, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=63\n",
            ">9/100, 75/81, d1=0.664, d2=0.683, g=0.758, secs=307, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=64\n",
            ">9/100, 80/81, d1=0.645, d2=0.684, g=0.771, secs=309, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=64\n",
            ">10/100, 5/81, d1=0.653, d2=0.652, g=0.807, secs=312, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=65\n",
            ">10/100, 10/81, d1=0.648, d2=0.701, g=0.767, secs=314, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=65\n",
            ">10/100, 15/81, d1=0.655, d2=0.729, g=0.735, secs=316, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=66\n",
            ">10/100, 20/81, d1=0.665, d2=0.725, g=0.719, secs=318, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=67\n",
            ">10/100, 25/81, d1=0.665, d2=0.719, g=0.735, secs=320, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=67\n",
            ">10/100, 30/81, d1=0.668, d2=0.697, g=0.763, secs=322, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=68\n",
            ">10/100, 35/81, d1=0.652, d2=0.678, g=0.764, secs=324, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=68\n",
            ">10/100, 40/81, d1=0.621, d2=0.678, g=0.765, secs=326, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=69\n",
            ">10/100, 45/81, d1=0.629, d2=0.693, g=0.730, secs=328, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=69\n",
            ">10/100, 50/81, d1=0.634, d2=0.736, g=0.723, secs=330, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=70\n",
            ">10/100, 55/81, d1=0.645, d2=0.750, g=0.707, secs=333, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=70\n",
            ">10/100, 60/81, d1=0.659, d2=0.734, g=0.732, secs=335, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=71\n",
            ">10/100, 65/81, d1=0.650, d2=0.712, g=0.749, secs=337, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=72\n",
            ">10/100, 70/81, d1=0.636, d2=0.697, g=0.744, secs=339, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=72\n",
            ">10/100, 75/81, d1=0.624, d2=0.681, g=0.765, secs=341, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=73\n",
            ">10/100, 80/81, d1=0.652, d2=0.672, g=0.763, secs=343, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=73\n",
            ">Accuracy real: 86%, fake: 94%\n",
            ">11/100, 5/81, d1=0.657, d2=0.683, g=0.769, secs=349, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=74\n",
            ">11/100, 10/81, d1=0.657, d2=0.688, g=0.778, secs=351, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=74\n",
            ">11/100, 15/81, d1=0.673, d2=0.688, g=0.766, secs=353, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=75\n",
            ">11/100, 20/81, d1=0.660, d2=0.682, g=0.777, secs=355, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=76\n",
            ">11/100, 25/81, d1=0.652, d2=0.667, g=0.778, secs=357, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=76\n",
            ">11/100, 30/81, d1=0.637, d2=0.665, g=0.793, secs=359, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=77\n",
            ">11/100, 35/81, d1=0.658, d2=0.671, g=0.802, secs=362, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=77\n",
            ">11/100, 40/81, d1=0.632, d2=0.709, g=0.760, secs=364, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=78\n",
            ">11/100, 45/81, d1=0.651, d2=0.686, g=0.773, secs=366, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=78\n",
            ">11/100, 50/81, d1=0.637, d2=0.690, g=0.755, secs=368, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=79\n",
            ">11/100, 55/81, d1=0.654, d2=0.691, g=0.769, secs=370, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=79\n",
            ">11/100, 60/81, d1=0.623, d2=0.692, g=0.751, secs=372, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=80\n",
            ">11/100, 65/81, d1=0.623, d2=0.697, g=0.749, secs=374, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=81\n",
            ">11/100, 70/81, d1=0.638, d2=0.694, g=0.738, secs=376, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=81\n",
            ">11/100, 75/81, d1=0.636, d2=0.688, g=0.757, secs=378, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=82\n",
            ">11/100, 80/81, d1=0.634, d2=0.688, g=0.761, secs=380, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=82\n",
            ">12/100, 5/81, d1=0.637, d2=0.669, g=0.790, secs=383, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=83\n",
            ">12/100, 10/81, d1=0.646, d2=0.659, g=0.795, secs=385, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=83\n",
            ">12/100, 15/81, d1=0.649, d2=0.673, g=0.802, secs=387, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=84\n",
            ">12/100, 20/81, d1=0.688, d2=0.667, g=0.798, secs=389, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=85\n",
            ">12/100, 25/81, d1=0.679, d2=0.667, g=0.799, secs=391, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=85\n",
            ">12/100, 30/81, d1=0.657, d2=0.679, g=0.782, secs=393, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=86\n",
            ">12/100, 35/81, d1=0.643, d2=0.673, g=0.781, secs=395, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=86\n",
            ">12/100, 40/81, d1=0.617, d2=0.690, g=0.767, secs=398, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=87\n",
            ">12/100, 45/81, d1=0.659, d2=0.689, g=0.773, secs=400, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=87\n",
            ">12/100, 50/81, d1=0.649, d2=0.700, g=0.751, secs=402, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=88\n",
            ">12/100, 55/81, d1=0.654, d2=0.700, g=0.766, secs=404, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=88\n",
            ">12/100, 60/81, d1=0.673, d2=0.676, g=0.767, secs=406, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=89\n",
            ">12/100, 65/81, d1=0.642, d2=0.690, g=0.767, secs=408, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=90\n",
            ">12/100, 70/81, d1=0.644, d2=0.680, g=0.771, secs=410, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=90\n",
            ">12/100, 75/81, d1=0.656, d2=0.666, g=0.792, secs=412, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=91\n",
            ">12/100, 80/81, d1=0.659, d2=0.674, g=0.789, secs=414, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=91\n",
            ">13/100, 5/81, d1=0.647, d2=0.728, g=0.747, secs=417, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=92\n",
            ">13/100, 10/81, d1=0.636, d2=0.731, g=0.735, secs=419, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=92\n",
            ">13/100, 15/81, d1=0.659, d2=0.720, g=0.760, secs=421, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=93\n",
            ">13/100, 20/81, d1=0.666, d2=0.698, g=0.763, secs=423, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=94\n",
            ">13/100, 25/81, d1=0.668, d2=0.675, g=0.768, secs=425, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=94\n",
            ">13/100, 30/81, d1=0.659, d2=0.677, g=0.769, secs=427, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=95\n",
            ">13/100, 35/81, d1=0.646, d2=0.685, g=0.766, secs=429, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=95\n",
            ">13/100, 40/81, d1=0.640, d2=0.707, g=0.765, secs=431, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=96\n",
            ">13/100, 45/81, d1=0.667, d2=0.704, g=0.763, secs=434, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=96\n",
            ">13/100, 50/81, d1=0.657, d2=0.695, g=0.747, secs=436, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=97\n",
            ">13/100, 55/81, d1=0.669, d2=0.697, g=0.747, secs=438, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=97\n",
            ">13/100, 60/81, d1=0.668, d2=0.693, g=0.759, secs=440, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=98\n",
            ">13/100, 65/81, d1=0.660, d2=0.688, g=0.767, secs=442, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=99\n",
            ">13/100, 70/81, d1=0.653, d2=0.714, g=0.736, secs=444, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=99\n",
            ">13/100, 75/81, d1=0.643, d2=0.686, g=0.756, secs=446, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=100\n",
            ">13/100, 80/81, d1=0.665, d2=0.684, g=0.770, secs=448, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=100\n",
            ">14/100, 5/81, d1=0.661, d2=0.678, g=0.774, secs=451, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=101\n",
            ">14/100, 10/81, d1=0.654, d2=0.689, g=0.767, secs=453, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=101\n",
            ">14/100, 15/81, d1=0.641, d2=0.691, g=0.776, secs=455, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=102\n",
            ">14/100, 20/81, d1=0.643, d2=0.685, g=0.770, secs=457, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=103\n",
            ">14/100, 25/81, d1=0.635, d2=0.695, g=0.769, secs=459, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=103\n",
            ">14/100, 30/81, d1=0.628, d2=0.706, g=0.769, secs=461, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=104\n",
            ">14/100, 35/81, d1=0.556, d2=0.781, g=0.661, secs=463, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=104\n",
            ">14/100, 40/81, d1=0.564, d2=0.797, g=0.653, secs=465, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=105\n",
            ">14/100, 45/81, d1=0.570, d2=0.753, g=0.685, secs=467, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=105\n",
            ">14/100, 50/81, d1=0.589, d2=0.737, g=0.715, secs=469, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=106\n",
            ">14/100, 55/81, d1=0.629, d2=0.702, g=0.747, secs=472, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=106\n",
            ">14/100, 60/81, d1=0.611, d2=0.705, g=0.779, secs=474, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=107\n",
            ">14/100, 65/81, d1=0.645, d2=0.695, g=0.766, secs=476, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=108\n",
            ">14/100, 70/81, d1=0.641, d2=0.708, g=0.764, secs=478, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=108\n",
            ">14/100, 75/81, d1=0.625, d2=0.713, g=0.744, secs=480, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=109\n",
            ">14/100, 80/81, d1=0.626, d2=0.704, g=0.730, secs=482, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=109\n",
            ">15/100, 5/81, d1=0.643, d2=0.703, g=0.742, secs=485, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=110\n",
            ">15/100, 10/81, d1=0.622, d2=0.705, g=0.746, secs=487, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=110\n",
            ">15/100, 15/81, d1=0.636, d2=0.702, g=0.742, secs=489, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=111\n",
            ">15/100, 20/81, d1=0.640, d2=0.693, g=0.768, secs=491, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=112\n",
            ">15/100, 25/81, d1=0.628, d2=0.693, g=0.769, secs=493, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=112\n",
            ">15/100, 30/81, d1=0.664, d2=0.667, g=0.782, secs=495, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=113\n",
            ">15/100, 35/81, d1=0.644, d2=0.695, g=0.768, secs=497, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=113\n",
            ">15/100, 40/81, d1=0.634, d2=0.701, g=0.784, secs=499, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=114\n",
            ">15/100, 45/81, d1=0.654, d2=0.668, g=0.796, secs=501, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=114\n",
            ">15/100, 50/81, d1=0.631, d2=0.664, g=0.808, secs=503, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=115\n",
            ">15/100, 55/81, d1=0.644, d2=0.691, g=0.775, secs=505, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=115\n",
            ">15/100, 60/81, d1=0.620, d2=0.675, g=0.780, secs=508, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=116\n",
            ">15/100, 65/81, d1=0.644, d2=0.663, g=0.787, secs=510, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=117\n",
            ">15/100, 70/81, d1=0.673, d2=0.692, g=0.765, secs=512, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=117\n",
            ">15/100, 75/81, d1=0.630, d2=0.681, g=0.776, secs=514, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=118\n",
            ">15/100, 80/81, d1=0.656, d2=0.678, g=0.778, secs=516, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=118\n",
            ">16/100, 5/81, d1=0.643, d2=0.688, g=0.802, secs=518, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=119\n",
            ">16/100, 10/81, d1=0.646, d2=0.709, g=0.771, secs=520, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=119\n",
            ">16/100, 15/81, d1=0.682, d2=0.707, g=0.748, secs=523, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=120\n",
            ">16/100, 20/81, d1=0.672, d2=0.703, g=0.758, secs=525, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=121\n",
            ">16/100, 25/81, d1=0.672, d2=0.682, g=0.767, secs=527, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=121\n",
            ">16/100, 30/81, d1=0.657, d2=0.678, g=0.769, secs=529, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=122\n",
            ">16/100, 35/81, d1=0.676, d2=0.689, g=0.770, secs=531, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=122\n",
            ">16/100, 40/81, d1=0.655, d2=0.683, g=0.764, secs=533, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=123\n",
            ">16/100, 45/81, d1=0.663, d2=0.705, g=0.758, secs=535, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=123\n",
            ">16/100, 50/81, d1=0.685, d2=0.679, g=0.748, secs=537, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=124\n",
            ">16/100, 55/81, d1=0.632, d2=0.694, g=0.756, secs=539, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=124\n",
            ">16/100, 60/81, d1=0.627, d2=0.683, g=0.770, secs=541, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=125\n",
            ">16/100, 65/81, d1=0.651, d2=0.706, g=0.754, secs=543, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=126\n",
            ">16/100, 70/81, d1=0.604, d2=0.732, g=0.746, secs=546, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=126\n",
            ">16/100, 75/81, d1=0.602, d2=0.720, g=0.730, secs=548, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=127\n",
            ">16/100, 80/81, d1=0.627, d2=0.704, g=0.759, secs=550, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=127\n",
            ">17/100, 5/81, d1=0.658, d2=0.682, g=0.797, secs=552, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=128\n",
            ">17/100, 10/81, d1=0.673, d2=0.671, g=0.797, secs=554, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=128\n",
            ">17/100, 15/81, d1=0.664, d2=0.664, g=0.808, secs=556, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=129\n",
            ">17/100, 20/81, d1=0.631, d2=0.681, g=0.794, secs=559, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=130\n",
            ">17/100, 25/81, d1=0.665, d2=0.684, g=0.782, secs=561, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=130\n",
            ">17/100, 30/81, d1=0.641, d2=0.686, g=0.771, secs=563, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=131\n",
            ">17/100, 35/81, d1=0.635, d2=0.705, g=0.756, secs=565, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=131\n",
            ">17/100, 40/81, d1=0.639, d2=0.708, g=0.754, secs=567, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=132\n",
            ">17/100, 45/81, d1=0.643, d2=0.687, g=0.763, secs=569, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=132\n",
            ">17/100, 50/81, d1=0.640, d2=0.692, g=0.756, secs=571, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=133\n",
            ">17/100, 55/81, d1=0.634, d2=0.681, g=0.772, secs=573, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=133\n",
            ">17/100, 60/81, d1=0.629, d2=0.694, g=0.766, secs=575, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=134\n",
            ">17/100, 65/81, d1=0.629, d2=0.694, g=0.768, secs=577, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=135\n",
            ">17/100, 70/81, d1=0.650, d2=0.686, g=0.787, secs=579, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=135\n",
            ">17/100, 75/81, d1=0.657, d2=0.672, g=0.801, secs=582, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=136\n",
            ">17/100, 80/81, d1=0.677, d2=0.678, g=0.795, secs=584, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=136\n",
            ">18/100, 5/81, d1=0.633, d2=0.687, g=0.775, secs=586, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=137\n",
            ">18/100, 10/81, d1=0.598, d2=0.644, g=0.856, secs=588, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=137\n",
            ">18/100, 15/81, d1=0.476, d2=0.608, g=0.871, secs=590, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=138\n",
            ">18/100, 20/81, d1=0.456, d2=0.736, g=0.745, secs=592, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=139\n",
            ">18/100, 25/81, d1=0.486, d2=0.845, g=0.682, secs=594, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=139\n",
            ">18/100, 30/81, d1=0.560, d2=0.766, g=0.728, secs=597, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=140\n",
            ">18/100, 35/81, d1=0.620, d2=0.792, g=0.679, secs=599, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=140\n",
            ">18/100, 40/81, d1=0.547, d2=0.807, g=0.675, secs=601, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=141\n",
            ">18/100, 45/81, d1=0.606, d2=0.770, g=0.717, secs=603, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=141\n",
            ">18/100, 50/81, d1=0.638, d2=0.701, g=0.751, secs=605, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=142\n",
            ">18/100, 55/81, d1=0.623, d2=0.720, g=0.775, secs=607, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=142\n",
            ">18/100, 60/81, d1=0.630, d2=0.702, g=0.778, secs=609, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=143\n",
            ">18/100, 65/81, d1=0.671, d2=0.688, g=0.796, secs=611, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=144\n",
            ">18/100, 70/81, d1=0.663, d2=0.677, g=0.802, secs=613, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=144\n",
            ">18/100, 75/81, d1=0.653, d2=0.670, g=0.808, secs=615, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=145\n",
            ">18/100, 80/81, d1=0.684, d2=0.646, g=0.810, secs=617, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=145\n",
            ">19/100, 5/81, d1=0.646, d2=0.669, g=0.776, secs=620, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=146\n",
            ">19/100, 10/81, d1=0.655, d2=0.691, g=0.784, secs=622, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=146\n",
            ">19/100, 15/81, d1=0.617, d2=0.694, g=0.771, secs=624, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=147\n",
            ">19/100, 20/81, d1=0.662, d2=0.705, g=0.770, secs=626, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=148\n",
            ">19/100, 25/81, d1=0.665, d2=0.692, g=0.770, secs=628, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=148\n",
            ">19/100, 30/81, d1=0.629, d2=0.687, g=0.789, secs=630, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=149\n",
            ">19/100, 35/81, d1=0.671, d2=0.693, g=0.763, secs=632, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=149\n",
            ">19/100, 40/81, d1=0.625, d2=0.718, g=0.734, secs=635, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=150\n",
            ">19/100, 45/81, d1=0.612, d2=0.673, g=0.799, secs=637, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=150\n",
            ">19/100, 50/81, d1=0.571, d2=0.722, g=0.740, secs=639, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=151\n",
            ">19/100, 55/81, d1=0.617, d2=0.707, g=0.767, secs=641, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=151\n",
            ">19/100, 60/81, d1=0.630, d2=0.670, g=0.791, secs=643, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=152\n",
            ">19/100, 65/81, d1=0.648, d2=0.674, g=0.788, secs=645, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=153\n",
            ">19/100, 70/81, d1=0.629, d2=0.681, g=0.786, secs=647, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=153\n",
            ">19/100, 75/81, d1=0.626, d2=0.700, g=0.783, secs=649, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=154\n",
            ">19/100, 80/81, d1=0.685, d2=0.668, g=0.787, secs=651, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=154\n",
            ">20/100, 5/81, d1=0.663, d2=0.668, g=0.794, secs=654, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=155\n",
            ">20/100, 10/81, d1=0.646, d2=0.675, g=0.789, secs=656, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=155\n",
            ">20/100, 15/81, d1=0.639, d2=0.692, g=0.780, secs=658, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=156\n",
            ">20/100, 20/81, d1=0.675, d2=0.696, g=0.765, secs=660, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=157\n",
            ">20/100, 25/81, d1=0.643, d2=0.684, g=0.789, secs=662, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=157\n",
            ">20/100, 30/81, d1=0.623, d2=0.688, g=0.756, secs=664, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=158\n",
            ">20/100, 35/81, d1=0.657, d2=0.682, g=0.789, secs=666, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=158\n",
            ">20/100, 40/81, d1=0.654, d2=0.704, g=0.780, secs=668, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=159\n",
            ">20/100, 45/81, d1=0.661, d2=0.680, g=0.768, secs=670, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=159\n",
            ">20/100, 50/81, d1=0.656, d2=0.698, g=0.782, secs=673, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=160\n",
            ">20/100, 55/81, d1=0.683, d2=0.679, g=0.773, secs=675, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=160\n",
            ">20/100, 60/81, d1=0.680, d2=0.684, g=0.780, secs=677, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=161\n",
            ">20/100, 65/81, d1=0.676, d2=0.678, g=0.783, secs=679, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=162\n",
            ">20/100, 70/81, d1=0.639, d2=0.696, g=0.781, secs=681, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=162\n",
            ">20/100, 75/81, d1=0.644, d2=0.683, g=0.790, secs=683, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=163\n",
            ">20/100, 80/81, d1=0.624, d2=0.690, g=0.784, secs=685, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=163\n",
            ">Accuracy real: 72%, fake: 92%\n",
            ">21/100, 5/81, d1=0.609, d2=0.689, g=0.787, secs=691, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=164\n",
            ">21/100, 10/81, d1=0.629, d2=0.700, g=0.762, secs=693, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=164\n",
            ">21/100, 15/81, d1=0.620, d2=0.719, g=0.760, secs=695, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=165\n",
            ">21/100, 20/81, d1=0.633, d2=0.728, g=0.750, secs=697, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=166\n",
            ">21/100, 25/81, d1=0.666, d2=0.709, g=0.766, secs=699, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=166\n",
            ">21/100, 30/81, d1=0.642, d2=0.701, g=0.775, secs=701, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=167\n",
            ">21/100, 35/81, d1=0.679, d2=0.692, g=0.773, secs=703, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=167\n",
            ">21/100, 40/81, d1=0.653, d2=0.680, g=0.800, secs=705, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=168\n",
            ">21/100, 45/81, d1=0.685, d2=0.659, g=0.816, secs=707, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=168\n",
            ">21/100, 50/81, d1=0.673, d2=0.655, g=0.809, secs=709, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=169\n",
            ">21/100, 55/81, d1=0.658, d2=0.664, g=0.818, secs=711, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=169\n",
            ">21/100, 60/81, d1=0.665, d2=0.673, g=0.796, secs=714, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=170\n",
            ">21/100, 65/81, d1=0.634, d2=0.694, g=0.794, secs=716, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=171\n",
            ">21/100, 70/81, d1=0.630, d2=0.688, g=0.806, secs=718, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=171\n",
            ">21/100, 75/81, d1=0.674, d2=0.651, g=0.818, secs=720, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=172\n",
            ">21/100, 80/81, d1=0.671, d2=0.644, g=0.832, secs=722, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=172\n",
            ">22/100, 5/81, d1=0.703, d2=0.661, g=0.814, secs=724, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=173\n",
            ">22/100, 10/81, d1=0.671, d2=0.646, g=0.820, secs=727, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=173\n",
            ">22/100, 15/81, d1=0.661, d2=0.680, g=0.804, secs=729, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=174\n",
            ">22/100, 20/81, d1=0.652, d2=0.680, g=0.786, secs=731, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=175\n",
            ">22/100, 25/81, d1=0.642, d2=0.677, g=0.797, secs=733, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=175\n",
            ">22/100, 30/81, d1=0.622, d2=0.668, g=0.783, secs=735, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=176\n",
            ">22/100, 35/81, d1=0.633, d2=0.676, g=0.788, secs=737, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=176\n",
            ">22/100, 40/81, d1=0.634, d2=0.667, g=0.794, secs=739, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=177\n",
            ">22/100, 45/81, d1=0.648, d2=0.689, g=0.772, secs=741, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=177\n",
            ">22/100, 50/81, d1=0.628, d2=0.695, g=0.774, secs=743, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=178\n",
            ">22/100, 55/81, d1=0.632, d2=0.700, g=0.787, secs=745, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=178\n",
            ">22/100, 60/81, d1=0.671, d2=0.672, g=0.791, secs=747, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=179\n",
            ">22/100, 65/81, d1=0.651, d2=0.675, g=0.802, secs=749, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=180\n",
            ">22/100, 70/81, d1=0.609, d2=0.674, g=0.766, secs=752, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=180\n",
            ">22/100, 75/81, d1=0.623, d2=0.688, g=0.764, secs=754, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=181\n",
            ">22/100, 80/81, d1=0.652, d2=0.686, g=0.791, secs=756, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=181\n",
            ">23/100, 5/81, d1=0.664, d2=0.671, g=0.794, secs=758, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=182\n",
            ">23/100, 10/81, d1=0.663, d2=0.663, g=0.793, secs=760, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=182\n",
            ">23/100, 15/81, d1=0.624, d2=0.674, g=0.806, secs=762, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=183\n",
            ">23/100, 20/81, d1=0.690, d2=0.686, g=0.803, secs=764, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=184\n",
            ">23/100, 25/81, d1=0.633, d2=0.684, g=0.802, secs=767, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=184\n",
            ">23/100, 30/81, d1=0.673, d2=0.652, g=0.824, secs=769, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=185\n",
            ">23/100, 35/81, d1=0.684, d2=0.680, g=0.820, secs=771, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=185\n",
            ">23/100, 40/81, d1=0.650, d2=0.632, g=0.828, secs=773, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=186\n",
            ">23/100, 45/81, d1=0.658, d2=0.661, g=0.831, secs=775, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=186\n",
            ">23/100, 50/81, d1=0.698, d2=0.659, g=0.810, secs=777, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=187\n",
            ">23/100, 55/81, d1=0.657, d2=0.647, g=0.825, secs=779, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=187\n",
            ">23/100, 60/81, d1=0.663, d2=0.633, g=0.822, secs=781, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=188\n",
            ">23/100, 65/81, d1=0.664, d2=0.668, g=0.824, secs=783, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=189\n",
            ">23/100, 70/81, d1=0.648, d2=0.669, g=0.812, secs=785, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=189\n",
            ">23/100, 75/81, d1=0.646, d2=0.679, g=0.815, secs=787, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=190\n",
            ">23/100, 80/81, d1=0.682, d2=0.672, g=0.811, secs=790, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=190\n",
            ">24/100, 5/81, d1=0.596, d2=0.671, g=0.809, secs=792, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=191\n",
            ">24/100, 10/81, d1=0.642, d2=0.684, g=0.813, secs=794, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=191\n",
            ">24/100, 15/81, d1=0.655, d2=0.652, g=0.820, secs=796, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=192\n",
            ">24/100, 20/81, d1=0.595, d2=0.673, g=0.817, secs=798, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=193\n",
            ">24/100, 25/81, d1=0.662, d2=0.665, g=0.839, secs=800, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=193\n",
            ">24/100, 30/81, d1=0.651, d2=0.656, g=0.842, secs=802, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=194\n",
            ">24/100, 35/81, d1=0.637, d2=0.663, g=0.819, secs=805, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=194\n",
            ">24/100, 40/81, d1=0.634, d2=0.679, g=0.792, secs=807, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=195\n",
            ">24/100, 45/81, d1=0.640, d2=0.689, g=0.817, secs=809, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=195\n",
            ">24/100, 50/81, d1=0.647, d2=0.698, g=0.825, secs=811, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=196\n",
            ">24/100, 55/81, d1=0.695, d2=0.663, g=0.830, secs=813, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=196\n",
            ">24/100, 60/81, d1=0.690, d2=0.674, g=0.818, secs=815, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=197\n",
            ">24/100, 65/81, d1=0.681, d2=0.681, g=0.805, secs=817, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=198\n",
            ">24/100, 70/81, d1=0.672, d2=0.667, g=0.790, secs=819, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=198\n",
            ">24/100, 75/81, d1=0.687, d2=0.683, g=0.825, secs=821, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=199\n",
            ">24/100, 80/81, d1=0.660, d2=0.672, g=0.820, secs=823, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=199\n",
            ">25/100, 5/81, d1=0.647, d2=0.653, g=0.805, secs=826, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=200\n",
            ">25/100, 10/81, d1=0.670, d2=0.667, g=0.818, secs=828, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=200\n",
            ">25/100, 15/81, d1=0.683, d2=0.661, g=0.806, secs=830, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=201\n",
            ">25/100, 20/81, d1=0.672, d2=0.689, g=0.795, secs=832, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=202\n",
            ">25/100, 25/81, d1=0.688, d2=0.642, g=0.828, secs=834, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=202\n",
            ">25/100, 30/81, d1=0.668, d2=0.677, g=0.811, secs=836, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=203\n",
            ">25/100, 35/81, d1=0.634, d2=0.669, g=0.795, secs=838, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=203\n",
            ">25/100, 40/81, d1=0.610, d2=0.680, g=0.803, secs=840, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=204\n",
            ">25/100, 45/81, d1=0.657, d2=0.693, g=0.803, secs=842, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=204\n",
            ">25/100, 50/81, d1=0.631, d2=0.693, g=0.805, secs=845, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=205\n",
            ">25/100, 55/81, d1=0.687, d2=0.682, g=0.778, secs=847, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=205\n",
            ">25/100, 60/81, d1=0.633, d2=0.703, g=0.780, secs=849, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=206\n",
            ">25/100, 65/81, d1=0.633, d2=0.686, g=0.775, secs=851, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=207\n",
            ">25/100, 70/81, d1=0.642, d2=0.698, g=0.778, secs=853, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=207\n",
            ">25/100, 75/81, d1=0.664, d2=0.702, g=0.785, secs=855, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=208\n",
            ">25/100, 80/81, d1=0.667, d2=0.671, g=0.805, secs=857, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=208\n",
            ">26/100, 5/81, d1=0.685, d2=0.679, g=0.781, secs=860, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=209\n",
            ">26/100, 10/81, d1=0.648, d2=0.680, g=0.781, secs=862, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=209\n",
            ">26/100, 15/81, d1=0.675, d2=0.679, g=0.785, secs=864, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=210\n",
            ">26/100, 20/81, d1=0.651, d2=0.692, g=0.778, secs=866, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=211\n",
            ">26/100, 25/81, d1=0.640, d2=0.674, g=0.814, secs=868, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=211\n",
            ">26/100, 30/81, d1=0.669, d2=0.677, g=0.793, secs=870, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=212\n",
            ">26/100, 35/81, d1=0.644, d2=0.659, g=0.806, secs=872, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=212\n",
            ">26/100, 40/81, d1=0.672, d2=0.672, g=0.792, secs=874, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=213\n",
            ">26/100, 45/81, d1=0.670, d2=0.651, g=0.823, secs=876, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=213\n",
            ">26/100, 50/81, d1=0.671, d2=0.645, g=0.823, secs=878, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=214\n",
            ">26/100, 55/81, d1=0.674, d2=0.690, g=0.801, secs=880, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=214\n",
            ">26/100, 60/81, d1=0.670, d2=0.660, g=0.809, secs=883, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=215\n",
            ">26/100, 65/81, d1=0.647, d2=0.675, g=0.801, secs=885, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=216\n",
            ">26/100, 70/81, d1=0.622, d2=0.692, g=0.787, secs=887, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=216\n",
            ">26/100, 75/81, d1=0.686, d2=0.688, g=0.795, secs=889, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=217\n",
            ">26/100, 80/81, d1=0.669, d2=0.682, g=0.788, secs=891, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=217\n",
            ">27/100, 5/81, d1=0.637, d2=0.687, g=0.778, secs=893, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=218\n",
            ">27/100, 10/81, d1=0.653, d2=0.699, g=0.801, secs=895, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=218\n",
            ">27/100, 15/81, d1=0.721, d2=0.666, g=0.798, secs=898, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=219\n",
            ">27/100, 20/81, d1=0.683, d2=0.683, g=0.800, secs=900, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=220\n",
            ">27/100, 25/81, d1=0.698, d2=0.685, g=0.785, secs=902, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=220\n",
            ">27/100, 30/81, d1=0.672, d2=0.660, g=0.806, secs=904, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=221\n",
            ">27/100, 35/81, d1=0.664, d2=0.654, g=0.823, secs=906, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=221\n",
            ">27/100, 40/81, d1=0.631, d2=0.690, g=0.778, secs=908, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=222\n",
            ">27/100, 45/81, d1=0.678, d2=0.679, g=0.794, secs=910, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=222\n",
            ">27/100, 50/81, d1=0.679, d2=0.680, g=0.804, secs=912, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=223\n",
            ">27/100, 55/81, d1=0.695, d2=0.662, g=0.816, secs=914, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=223\n",
            ">27/100, 60/81, d1=0.659, d2=0.672, g=0.822, secs=916, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=224\n",
            ">27/100, 65/81, d1=0.679, d2=0.664, g=0.811, secs=918, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=225\n",
            ">27/100, 70/81, d1=0.648, d2=0.638, g=0.810, secs=920, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=225\n",
            ">27/100, 75/81, d1=0.694, d2=0.663, g=0.791, secs=923, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=226\n",
            ">27/100, 80/81, d1=0.671, d2=0.677, g=0.774, secs=925, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=226\n",
            ">28/100, 5/81, d1=0.681, d2=0.667, g=0.797, secs=927, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=227\n",
            ">28/100, 10/81, d1=0.691, d2=0.664, g=0.815, secs=929, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=227\n",
            ">28/100, 15/81, d1=0.641, d2=0.675, g=0.773, secs=931, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=228\n",
            ">28/100, 20/81, d1=0.661, d2=0.671, g=0.788, secs=933, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=229\n",
            ">28/100, 25/81, d1=0.638, d2=0.684, g=0.779, secs=935, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=229\n",
            ">28/100, 30/81, d1=0.660, d2=0.680, g=0.802, secs=938, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=230\n",
            ">28/100, 35/81, d1=0.659, d2=0.670, g=0.781, secs=940, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=230\n",
            ">28/100, 40/81, d1=0.696, d2=0.682, g=0.784, secs=942, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=231\n",
            ">28/100, 45/81, d1=0.644, d2=0.679, g=0.778, secs=944, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=231\n",
            ">28/100, 50/81, d1=0.658, d2=0.719, g=0.783, secs=946, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=232\n",
            ">28/100, 55/81, d1=0.671, d2=0.690, g=0.776, secs=948, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=232\n",
            ">28/100, 60/81, d1=0.678, d2=0.673, g=0.779, secs=950, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=233\n",
            ">28/100, 65/81, d1=0.656, d2=0.719, g=0.780, secs=952, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=234\n",
            ">28/100, 70/81, d1=0.639, d2=0.709, g=0.776, secs=954, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=234\n",
            ">28/100, 75/81, d1=0.654, d2=0.665, g=0.791, secs=956, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=235\n",
            ">28/100, 80/81, d1=0.705, d2=0.685, g=0.773, secs=958, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=235\n",
            ">29/100, 5/81, d1=0.660, d2=0.683, g=0.768, secs=961, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=236\n",
            ">29/100, 10/81, d1=0.659, d2=0.692, g=0.770, secs=963, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=236\n",
            ">29/100, 15/81, d1=0.628, d2=0.714, g=0.783, secs=965, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=237\n",
            ">29/100, 20/81, d1=0.646, d2=0.687, g=0.788, secs=967, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=238\n",
            ">29/100, 25/81, d1=0.665, d2=0.698, g=0.766, secs=969, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=238\n",
            ">29/100, 30/81, d1=0.664, d2=0.694, g=0.761, secs=971, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=239\n",
            ">29/100, 35/81, d1=0.680, d2=0.687, g=0.806, secs=973, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=239\n",
            ">29/100, 40/81, d1=0.638, d2=0.691, g=0.756, secs=976, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=240\n",
            ">29/100, 45/81, d1=0.650, d2=0.715, g=0.761, secs=978, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=240\n",
            ">29/100, 50/81, d1=0.654, d2=0.687, g=0.767, secs=980, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=241\n",
            ">29/100, 55/81, d1=0.610, d2=0.710, g=0.767, secs=982, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=241\n",
            ">29/100, 60/81, d1=0.651, d2=0.702, g=0.771, secs=984, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=242\n",
            ">29/100, 65/81, d1=0.666, d2=0.696, g=0.771, secs=986, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=243\n",
            ">29/100, 70/81, d1=0.654, d2=0.696, g=0.776, secs=988, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=243\n",
            ">29/100, 75/81, d1=0.680, d2=0.678, g=0.796, secs=990, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=244\n",
            ">29/100, 80/81, d1=0.682, d2=0.692, g=0.798, secs=992, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=244\n",
            ">30/100, 5/81, d1=0.639, d2=0.704, g=0.779, secs=995, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=245\n",
            ">30/100, 10/81, d1=0.664, d2=0.684, g=0.765, secs=997, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=245\n",
            ">30/100, 15/81, d1=0.646, d2=0.677, g=0.796, secs=999, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=246\n",
            ">30/100, 20/81, d1=0.685, d2=0.676, g=0.776, secs=1001, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=247\n",
            ">30/100, 25/81, d1=0.687, d2=0.670, g=0.792, secs=1003, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=247\n",
            ">30/100, 30/81, d1=0.661, d2=0.676, g=0.800, secs=1005, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=248\n",
            ">30/100, 35/81, d1=0.669, d2=0.681, g=0.781, secs=1007, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=248\n",
            ">30/100, 40/81, d1=0.629, d2=0.667, g=0.803, secs=1009, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=249\n",
            ">30/100, 45/81, d1=0.682, d2=0.660, g=0.804, secs=1011, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=249\n",
            ">30/100, 50/81, d1=0.712, d2=0.663, g=0.793, secs=1013, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=250\n",
            ">30/100, 55/81, d1=0.661, d2=0.678, g=0.792, secs=1015, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=250\n",
            ">30/100, 60/81, d1=0.642, d2=0.683, g=0.780, secs=1018, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=251\n",
            ">30/100, 65/81, d1=0.660, d2=0.693, g=0.767, secs=1020, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=252\n",
            ">30/100, 70/81, d1=0.661, d2=0.692, g=0.761, secs=1022, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=252\n",
            ">30/100, 75/81, d1=0.667, d2=0.687, g=0.769, secs=1024, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=253\n",
            ">30/100, 80/81, d1=0.649, d2=0.701, g=0.760, secs=1026, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=253\n",
            ">Accuracy real: 68%, fake: 70%\n",
            ">31/100, 5/81, d1=0.671, d2=0.710, g=0.756, secs=1031, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=254\n",
            ">31/100, 10/81, d1=0.682, d2=0.688, g=0.752, secs=1033, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=254\n",
            ">31/100, 15/81, d1=0.681, d2=0.691, g=0.749, secs=1036, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=255\n",
            ">31/100, 20/81, d1=0.659, d2=0.701, g=0.756, secs=1038, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=256\n",
            ">31/100, 25/81, d1=0.669, d2=0.687, g=0.769, secs=1040, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=256\n",
            ">31/100, 30/81, d1=0.663, d2=0.706, g=0.760, secs=1042, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=257\n",
            ">31/100, 35/81, d1=0.676, d2=0.696, g=0.734, secs=1044, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=257\n",
            ">31/100, 40/81, d1=0.644, d2=0.688, g=0.751, secs=1046, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=258\n",
            ">31/100, 45/81, d1=0.679, d2=0.702, g=0.753, secs=1048, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=258\n",
            ">31/100, 50/81, d1=0.677, d2=0.697, g=0.756, secs=1050, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=259\n",
            ">31/100, 55/81, d1=0.656, d2=0.706, g=0.770, secs=1052, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=259\n",
            ">31/100, 60/81, d1=0.684, d2=0.699, g=0.769, secs=1054, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=260\n",
            ">31/100, 65/81, d1=0.705, d2=0.687, g=0.761, secs=1056, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=261\n",
            ">31/100, 70/81, d1=0.659, d2=0.708, g=0.746, secs=1059, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=261\n",
            ">31/100, 75/81, d1=0.678, d2=0.687, g=0.737, secs=1061, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=262\n",
            ">31/100, 80/81, d1=0.665, d2=0.689, g=0.761, secs=1063, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=262\n",
            ">32/100, 5/81, d1=0.686, d2=0.718, g=0.742, secs=1065, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=263\n",
            ">32/100, 10/81, d1=0.676, d2=0.685, g=0.754, secs=1067, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=263\n",
            ">32/100, 15/81, d1=0.686, d2=0.694, g=0.756, secs=1069, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=264\n",
            ">32/100, 20/81, d1=0.671, d2=0.689, g=0.746, secs=1071, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=265\n",
            ">32/100, 25/81, d1=0.662, d2=0.698, g=0.752, secs=1074, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=265\n",
            ">32/100, 30/81, d1=0.655, d2=0.711, g=0.745, secs=1076, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=266\n",
            ">32/100, 35/81, d1=0.659, d2=0.701, g=0.734, secs=1078, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=266\n",
            ">32/100, 40/81, d1=0.654, d2=0.695, g=0.734, secs=1080, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=267\n",
            ">32/100, 45/81, d1=0.649, d2=0.701, g=0.741, secs=1082, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=267\n",
            ">32/100, 50/81, d1=0.656, d2=0.693, g=0.754, secs=1084, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=268\n",
            ">32/100, 55/81, d1=0.683, d2=0.686, g=0.775, secs=1086, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=268\n",
            ">32/100, 60/81, d1=0.657, d2=0.703, g=0.756, secs=1088, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=269\n",
            ">32/100, 65/81, d1=0.661, d2=0.681, g=0.759, secs=1090, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=270\n",
            ">32/100, 70/81, d1=0.670, d2=0.689, g=0.763, secs=1092, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=270\n",
            ">32/100, 75/81, d1=0.674, d2=0.671, g=0.779, secs=1094, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=271\n",
            ">32/100, 80/81, d1=0.682, d2=0.680, g=0.764, secs=1096, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=271\n",
            ">33/100, 5/81, d1=0.670, d2=0.682, g=0.772, secs=1099, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=272\n",
            ">33/100, 10/81, d1=0.685, d2=0.697, g=0.757, secs=1101, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=272\n",
            ">33/100, 15/81, d1=0.670, d2=0.676, g=0.765, secs=1103, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=273\n",
            ">33/100, 20/81, d1=0.663, d2=0.705, g=0.755, secs=1105, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=274\n",
            ">33/100, 25/81, d1=0.685, d2=0.683, g=0.772, secs=1107, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=274\n",
            ">33/100, 30/81, d1=0.671, d2=0.680, g=0.769, secs=1109, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=275\n",
            ">33/100, 35/81, d1=0.653, d2=0.685, g=0.763, secs=1111, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=275\n",
            ">33/100, 40/81, d1=0.652, d2=0.689, g=0.750, secs=1114, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=276\n",
            ">33/100, 45/81, d1=0.666, d2=0.699, g=0.759, secs=1116, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=276\n",
            ">33/100, 50/81, d1=0.660, d2=0.692, g=0.754, secs=1118, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=277\n",
            ">33/100, 55/81, d1=0.669, d2=0.689, g=0.744, secs=1120, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=277\n",
            ">33/100, 60/81, d1=0.691, d2=0.691, g=0.759, secs=1122, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=278\n",
            ">33/100, 65/81, d1=0.673, d2=0.689, g=0.771, secs=1124, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=279\n",
            ">33/100, 70/81, d1=0.669, d2=0.700, g=0.755, secs=1126, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=279\n",
            ">33/100, 75/81, d1=0.677, d2=0.696, g=0.754, secs=1128, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=280\n",
            ">33/100, 80/81, d1=0.668, d2=0.707, g=0.746, secs=1130, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=280\n",
            ">34/100, 5/81, d1=0.666, d2=0.704, g=0.749, secs=1133, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=281\n",
            ">34/100, 10/81, d1=0.675, d2=0.696, g=0.753, secs=1135, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=281\n",
            ">34/100, 15/81, d1=0.668, d2=0.709, g=0.749, secs=1137, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=282\n",
            ">34/100, 20/81, d1=0.668, d2=0.695, g=0.750, secs=1139, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=283\n",
            ">34/100, 25/81, d1=0.660, d2=0.689, g=0.753, secs=1141, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=283\n",
            ">34/100, 30/81, d1=0.683, d2=0.692, g=0.748, secs=1143, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=284\n",
            ">34/100, 35/81, d1=0.681, d2=0.692, g=0.762, secs=1145, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=284\n",
            ">34/100, 40/81, d1=0.681, d2=0.679, g=0.752, secs=1147, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=285\n",
            ">34/100, 45/81, d1=0.664, d2=0.706, g=0.745, secs=1149, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=285\n",
            ">34/100, 50/81, d1=0.669, d2=0.691, g=0.756, secs=1151, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=286\n",
            ">34/100, 55/81, d1=0.662, d2=0.685, g=0.755, secs=1154, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=286\n",
            ">34/100, 60/81, d1=0.691, d2=0.712, g=0.762, secs=1156, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=287\n",
            ">34/100, 65/81, d1=0.640, d2=0.685, g=0.748, secs=1158, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=288\n",
            ">34/100, 70/81, d1=0.656, d2=0.712, g=0.743, secs=1160, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=288\n",
            ">34/100, 75/81, d1=0.659, d2=0.687, g=0.746, secs=1162, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=289\n",
            ">34/100, 80/81, d1=0.679, d2=0.689, g=0.752, secs=1164, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=289\n",
            ">35/100, 5/81, d1=0.706, d2=0.664, g=0.766, secs=1166, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=290\n",
            ">35/100, 10/81, d1=0.679, d2=0.697, g=0.762, secs=1169, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=290\n",
            ">35/100, 15/81, d1=0.681, d2=0.699, g=0.748, secs=1171, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=291\n",
            ">35/100, 20/81, d1=0.677, d2=0.693, g=0.752, secs=1173, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=292\n",
            ">35/100, 25/81, d1=0.663, d2=0.697, g=0.743, secs=1175, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=292\n",
            ">35/100, 30/81, d1=0.654, d2=0.711, g=0.738, secs=1177, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=293\n",
            ">35/100, 35/81, d1=0.652, d2=0.695, g=0.752, secs=1179, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=293\n",
            ">35/100, 40/81, d1=0.687, d2=0.683, g=0.760, secs=1181, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=294\n",
            ">35/100, 45/81, d1=0.681, d2=0.683, g=0.753, secs=1183, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=294\n",
            ">35/100, 50/81, d1=0.661, d2=0.709, g=0.755, secs=1185, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=295\n",
            ">35/100, 55/81, d1=0.665, d2=0.696, g=0.753, secs=1187, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=295\n",
            ">35/100, 60/81, d1=0.655, d2=0.699, g=0.752, secs=1189, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=296\n",
            ">35/100, 65/81, d1=0.661, d2=0.687, g=0.743, secs=1191, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=297\n",
            ">35/100, 70/81, d1=0.646, d2=0.704, g=0.736, secs=1194, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=297\n",
            ">35/100, 75/81, d1=0.664, d2=0.701, g=0.749, secs=1196, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=298\n",
            ">35/100, 80/81, d1=0.675, d2=0.697, g=0.741, secs=1198, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=298\n",
            ">36/100, 5/81, d1=0.639, d2=0.685, g=0.745, secs=1200, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=299\n",
            ">36/100, 10/81, d1=0.649, d2=0.712, g=0.750, secs=1202, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=299\n",
            ">36/100, 15/81, d1=0.657, d2=0.696, g=0.755, secs=1204, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=300\n",
            ">36/100, 20/81, d1=0.656, d2=0.698, g=0.745, secs=1206, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=301\n",
            ">36/100, 25/81, d1=0.679, d2=0.689, g=0.760, secs=1209, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=301\n",
            ">36/100, 30/81, d1=0.674, d2=0.695, g=0.761, secs=1211, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=302\n",
            ">36/100, 35/81, d1=0.672, d2=0.693, g=0.775, secs=1213, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=302\n",
            ">36/100, 40/81, d1=0.657, d2=0.691, g=0.741, secs=1215, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=303\n",
            ">36/100, 45/81, d1=0.667, d2=0.701, g=0.736, secs=1217, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=303\n",
            ">36/100, 50/81, d1=0.637, d2=0.705, g=0.746, secs=1219, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=304\n",
            ">36/100, 55/81, d1=0.669, d2=0.695, g=0.733, secs=1221, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=304\n",
            ">36/100, 60/81, d1=0.679, d2=0.695, g=0.752, secs=1223, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=305\n",
            ">36/100, 65/81, d1=0.681, d2=0.713, g=0.742, secs=1225, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=306\n",
            ">36/100, 70/81, d1=0.694, d2=0.708, g=0.745, secs=1227, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=306\n",
            ">36/100, 75/81, d1=0.680, d2=0.708, g=0.757, secs=1229, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=307\n",
            ">36/100, 80/81, d1=0.686, d2=0.712, g=0.742, secs=1231, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=307\n",
            ">37/100, 5/81, d1=0.667, d2=0.684, g=0.742, secs=1234, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=308\n",
            ">37/100, 10/81, d1=0.668, d2=0.698, g=0.744, secs=1236, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=308\n",
            ">37/100, 15/81, d1=0.669, d2=0.715, g=0.731, secs=1238, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=309\n",
            ">37/100, 20/81, d1=0.672, d2=0.697, g=0.748, secs=1240, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=310\n",
            ">37/100, 25/81, d1=0.686, d2=0.704, g=0.755, secs=1242, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=310\n",
            ">37/100, 30/81, d1=0.691, d2=0.687, g=0.754, secs=1244, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=311\n",
            ">37/100, 35/81, d1=0.660, d2=0.698, g=0.763, secs=1246, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=311\n",
            ">37/100, 40/81, d1=0.674, d2=0.699, g=0.749, secs=1248, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=312\n",
            ">37/100, 45/81, d1=0.654, d2=0.703, g=0.744, secs=1251, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=312\n",
            ">37/100, 50/81, d1=0.642, d2=0.700, g=0.743, secs=1253, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=313\n",
            ">37/100, 55/81, d1=0.669, d2=0.687, g=0.747, secs=1255, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=313\n",
            ">37/100, 60/81, d1=0.671, d2=0.695, g=0.766, secs=1257, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=314\n",
            ">37/100, 65/81, d1=0.679, d2=0.676, g=0.759, secs=1259, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=315\n",
            ">37/100, 70/81, d1=0.641, d2=0.684, g=0.760, secs=1261, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=315\n",
            ">37/100, 75/81, d1=0.673, d2=0.697, g=0.760, secs=1263, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=316\n",
            ">37/100, 80/81, d1=0.709, d2=0.689, g=0.757, secs=1265, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=316\n",
            ">38/100, 5/81, d1=0.685, d2=0.690, g=0.743, secs=1268, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=317\n",
            ">38/100, 10/81, d1=0.654, d2=0.705, g=0.746, secs=1270, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=317\n",
            ">38/100, 15/81, d1=0.665, d2=0.691, g=0.749, secs=1272, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=318\n",
            ">38/100, 20/81, d1=0.633, d2=0.710, g=0.748, secs=1274, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=319\n",
            ">38/100, 25/81, d1=0.663, d2=0.698, g=0.755, secs=1276, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=319\n",
            ">38/100, 30/81, d1=0.664, d2=0.697, g=0.746, secs=1278, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=320\n",
            ">38/100, 35/81, d1=0.654, d2=0.694, g=0.758, secs=1280, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=320\n",
            ">38/100, 40/81, d1=0.673, d2=0.690, g=0.757, secs=1282, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=321\n",
            ">38/100, 45/81, d1=0.665, d2=0.696, g=0.735, secs=1284, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=321\n",
            ">38/100, 50/81, d1=0.652, d2=0.720, g=0.738, secs=1286, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=322\n",
            ">38/100, 55/81, d1=0.666, d2=0.712, g=0.736, secs=1288, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=322\n",
            ">38/100, 60/81, d1=0.682, d2=0.713, g=0.748, secs=1291, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=323\n",
            ">38/100, 65/81, d1=0.687, d2=0.684, g=0.747, secs=1293, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=324\n",
            ">38/100, 70/81, d1=0.673, d2=0.690, g=0.754, secs=1295, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=324\n",
            ">38/100, 75/81, d1=0.679, d2=0.693, g=0.749, secs=1297, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=325\n",
            ">38/100, 80/81, d1=0.661, d2=0.705, g=0.738, secs=1299, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=325\n",
            ">39/100, 5/81, d1=0.671, d2=0.695, g=0.751, secs=1301, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=326\n",
            ">39/100, 10/81, d1=0.676, d2=0.692, g=0.743, secs=1303, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=326\n",
            ">39/100, 15/81, d1=0.681, d2=0.697, g=0.746, secs=1306, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=327\n",
            ">39/100, 20/81, d1=0.668, d2=0.699, g=0.742, secs=1308, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=328\n",
            ">39/100, 25/81, d1=0.663, d2=0.690, g=0.748, secs=1310, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=328\n",
            ">39/100, 30/81, d1=0.657, d2=0.686, g=0.739, secs=1312, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=329\n",
            ">39/100, 35/81, d1=0.702, d2=0.712, g=0.744, secs=1314, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=329\n",
            ">39/100, 40/81, d1=0.672, d2=0.693, g=0.758, secs=1316, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=330\n",
            ">39/100, 45/81, d1=0.663, d2=0.706, g=0.751, secs=1318, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=330\n",
            ">39/100, 50/81, d1=0.682, d2=0.694, g=0.746, secs=1320, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=331\n",
            ">39/100, 55/81, d1=0.676, d2=0.684, g=0.766, secs=1322, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=331\n",
            ">39/100, 60/81, d1=0.654, d2=0.699, g=0.743, secs=1324, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=332\n",
            ">39/100, 65/81, d1=0.649, d2=0.695, g=0.741, secs=1326, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=333\n",
            ">39/100, 70/81, d1=0.669, d2=0.689, g=0.752, secs=1328, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=333\n",
            ">39/100, 75/81, d1=0.654, d2=0.689, g=0.746, secs=1331, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=334\n",
            ">39/100, 80/81, d1=0.671, d2=0.695, g=0.756, secs=1333, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=334\n",
            ">40/100, 5/81, d1=0.665, d2=0.696, g=0.763, secs=1335, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=335\n",
            ">40/100, 10/81, d1=0.683, d2=0.686, g=0.760, secs=1337, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=335\n",
            ">40/100, 15/81, d1=0.669, d2=0.708, g=0.737, secs=1339, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=336\n",
            ">40/100, 20/81, d1=0.674, d2=0.706, g=0.744, secs=1341, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=337\n",
            ">40/100, 25/81, d1=0.661, d2=0.710, g=0.752, secs=1343, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=337\n",
            ">40/100, 30/81, d1=0.661, d2=0.710, g=0.734, secs=1346, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=338\n",
            ">40/100, 35/81, d1=0.678, d2=0.710, g=0.754, secs=1348, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=338\n",
            ">40/100, 40/81, d1=0.673, d2=0.692, g=0.743, secs=1350, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=339\n",
            ">40/100, 45/81, d1=0.684, d2=0.690, g=0.731, secs=1352, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=339\n",
            ">40/100, 50/81, d1=0.683, d2=0.707, g=0.753, secs=1354, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=340\n",
            ">40/100, 55/81, d1=0.679, d2=0.673, g=0.758, secs=1356, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=340\n",
            ">40/100, 60/81, d1=0.649, d2=0.687, g=0.761, secs=1358, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=341\n",
            ">40/100, 65/81, d1=0.670, d2=0.709, g=0.732, secs=1360, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=342\n",
            ">40/100, 70/81, d1=0.656, d2=0.710, g=0.722, secs=1362, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=342\n",
            ">40/100, 75/81, d1=0.647, d2=0.710, g=0.731, secs=1364, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=343\n",
            ">40/100, 80/81, d1=0.644, d2=0.697, g=0.738, secs=1366, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=343\n",
            ">Accuracy real: 70%, fake: 86%\n",
            ">41/100, 5/81, d1=0.669, d2=0.695, g=0.728, secs=1372, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=344\n",
            ">41/100, 10/81, d1=0.676, d2=0.709, g=0.746, secs=1374, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=344\n",
            ">41/100, 15/81, d1=0.668, d2=0.703, g=0.746, secs=1376, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=345\n",
            ">41/100, 20/81, d1=0.680, d2=0.690, g=0.743, secs=1378, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=346\n",
            ">41/100, 25/81, d1=0.653, d2=0.703, g=0.732, secs=1380, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=346\n",
            ">41/100, 30/81, d1=0.659, d2=0.692, g=0.739, secs=1382, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=347\n",
            ">41/100, 35/81, d1=0.650, d2=0.708, g=0.741, secs=1385, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=347\n",
            ">41/100, 40/81, d1=0.668, d2=0.710, g=0.726, secs=1387, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=348\n",
            ">41/100, 45/81, d1=0.656, d2=0.695, g=0.734, secs=1389, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=348\n",
            ">41/100, 50/81, d1=0.681, d2=0.686, g=0.757, secs=1391, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=349\n",
            ">41/100, 55/81, d1=0.673, d2=0.670, g=0.768, secs=1393, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=349\n",
            ">41/100, 60/81, d1=0.696, d2=0.667, g=0.773, secs=1395, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=350\n",
            ">41/100, 65/81, d1=0.677, d2=0.683, g=0.754, secs=1397, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=351\n",
            ">41/100, 70/81, d1=0.658, d2=0.698, g=0.732, secs=1399, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=351\n",
            ">41/100, 75/81, d1=0.663, d2=0.707, g=0.730, secs=1401, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=352\n",
            ">41/100, 80/81, d1=0.672, d2=0.703, g=0.742, secs=1403, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=352\n",
            ">42/100, 5/81, d1=0.682, d2=0.692, g=0.741, secs=1406, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=353\n",
            ">42/100, 10/81, d1=0.669, d2=0.718, g=0.753, secs=1408, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=353\n",
            ">42/100, 15/81, d1=0.665, d2=0.689, g=0.738, secs=1410, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=354\n",
            ">42/100, 20/81, d1=0.671, d2=0.702, g=0.746, secs=1412, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=355\n",
            ">42/100, 25/81, d1=0.675, d2=0.699, g=0.750, secs=1414, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=355\n",
            ">42/100, 30/81, d1=0.676, d2=0.692, g=0.748, secs=1416, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=356\n",
            ">42/100, 35/81, d1=0.667, d2=0.720, g=0.732, secs=1418, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=356\n",
            ">42/100, 40/81, d1=0.675, d2=0.698, g=0.734, secs=1420, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=357\n",
            ">42/100, 45/81, d1=0.653, d2=0.697, g=0.730, secs=1422, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=357\n",
            ">42/100, 50/81, d1=0.679, d2=0.694, g=0.745, secs=1425, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=358\n",
            ">42/100, 55/81, d1=0.679, d2=0.694, g=0.753, secs=1427, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=358\n",
            ">42/100, 60/81, d1=0.676, d2=0.702, g=0.752, secs=1429, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=359\n",
            ">42/100, 65/81, d1=0.687, d2=0.676, g=0.748, secs=1431, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=360\n",
            ">42/100, 70/81, d1=0.667, d2=0.683, g=0.753, secs=1433, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=360\n",
            ">42/100, 75/81, d1=0.669, d2=0.690, g=0.748, secs=1435, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=361\n",
            ">42/100, 80/81, d1=0.647, d2=0.686, g=0.749, secs=1437, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=361\n",
            ">43/100, 5/81, d1=0.643, d2=0.699, g=0.738, secs=1440, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=362\n",
            ">43/100, 10/81, d1=0.635, d2=0.700, g=0.738, secs=1442, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=362\n",
            ">43/100, 15/81, d1=0.658, d2=0.692, g=0.752, secs=1444, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=363\n",
            ">43/100, 20/81, d1=0.677, d2=0.689, g=0.755, secs=1446, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=364\n",
            ">43/100, 25/81, d1=0.666, d2=0.685, g=0.752, secs=1448, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=364\n",
            ">43/100, 30/81, d1=0.632, d2=0.694, g=0.748, secs=1450, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=365\n",
            ">43/100, 35/81, d1=0.671, d2=0.683, g=0.756, secs=1452, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=365\n",
            ">43/100, 40/81, d1=0.637, d2=0.706, g=0.743, secs=1454, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=366\n",
            ">43/100, 45/81, d1=0.668, d2=0.689, g=0.745, secs=1456, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=366\n",
            ">43/100, 50/81, d1=0.640, d2=0.702, g=0.737, secs=1458, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=367\n",
            ">43/100, 55/81, d1=0.629, d2=0.720, g=0.745, secs=1461, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=367\n",
            ">43/100, 60/81, d1=0.666, d2=0.697, g=0.742, secs=1463, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=368\n",
            ">43/100, 65/81, d1=0.659, d2=0.708, g=0.740, secs=1465, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=369\n",
            ">43/100, 70/81, d1=0.659, d2=0.718, g=0.744, secs=1467, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=369\n",
            ">43/100, 75/81, d1=0.658, d2=0.703, g=0.745, secs=1469, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=370\n",
            ">43/100, 80/81, d1=0.647, d2=0.704, g=0.735, secs=1471, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=370\n",
            ">44/100, 5/81, d1=0.654, d2=0.719, g=0.740, secs=1473, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=371\n",
            ">44/100, 10/81, d1=0.641, d2=0.711, g=0.734, secs=1476, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=371\n",
            ">44/100, 15/81, d1=0.660, d2=0.705, g=0.730, secs=1478, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=372\n",
            ">44/100, 20/81, d1=0.672, d2=0.689, g=0.750, secs=1480, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=373\n",
            ">44/100, 25/81, d1=0.639, d2=0.710, g=0.738, secs=1482, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=373\n",
            ">44/100, 30/81, d1=0.645, d2=0.707, g=0.731, secs=1484, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=374\n",
            ">44/100, 35/81, d1=0.654, d2=0.696, g=0.725, secs=1486, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=374\n",
            ">44/100, 40/81, d1=0.640, d2=0.711, g=0.730, secs=1488, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=375\n",
            ">44/100, 45/81, d1=0.669, d2=0.699, g=0.742, secs=1490, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=375\n",
            ">44/100, 50/81, d1=0.670, d2=0.712, g=0.748, secs=1492, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=376\n",
            ">44/100, 55/81, d1=0.682, d2=0.702, g=0.746, secs=1494, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=376\n",
            ">44/100, 60/81, d1=0.656, d2=0.683, g=0.760, secs=1496, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=377\n",
            ">44/100, 65/81, d1=0.643, d2=0.703, g=0.740, secs=1498, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=378\n",
            ">44/100, 70/81, d1=0.639, d2=0.702, g=0.731, secs=1501, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=378\n",
            ">44/100, 75/81, d1=0.630, d2=0.697, g=0.745, secs=1503, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=379\n",
            ">44/100, 80/81, d1=0.633, d2=0.701, g=0.752, secs=1505, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=379\n",
            ">45/100, 5/81, d1=0.664, d2=0.694, g=0.745, secs=1507, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=380\n",
            ">45/100, 10/81, d1=0.640, d2=0.693, g=0.746, secs=1509, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=380\n",
            ">45/100, 15/81, d1=0.650, d2=0.698, g=0.734, secs=1511, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=381\n",
            ">45/100, 20/81, d1=0.649, d2=0.705, g=0.738, secs=1513, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=382\n",
            ">45/100, 25/81, d1=0.660, d2=0.711, g=0.727, secs=1516, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=382\n",
            ">45/100, 30/81, d1=0.648, d2=0.705, g=0.744, secs=1518, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=383\n",
            ">45/100, 35/81, d1=0.656, d2=0.704, g=0.736, secs=1520, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=383\n",
            ">45/100, 40/81, d1=0.642, d2=0.688, g=0.751, secs=1522, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=384\n",
            ">45/100, 45/81, d1=0.675, d2=0.682, g=0.755, secs=1524, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=384\n",
            ">45/100, 50/81, d1=0.678, d2=0.701, g=0.742, secs=1526, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=385\n",
            ">45/100, 55/81, d1=0.658, d2=0.700, g=0.739, secs=1528, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=385\n",
            ">45/100, 60/81, d1=0.667, d2=0.699, g=0.743, secs=1530, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=386\n",
            ">45/100, 65/81, d1=0.656, d2=0.690, g=0.755, secs=1532, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=387\n",
            ">45/100, 70/81, d1=0.668, d2=0.707, g=0.738, secs=1534, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=387\n",
            ">45/100, 75/81, d1=0.671, d2=0.692, g=0.743, secs=1536, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=388\n",
            ">45/100, 80/81, d1=0.653, d2=0.712, g=0.742, secs=1538, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=388\n",
            ">46/100, 5/81, d1=0.674, d2=0.693, g=0.737, secs=1541, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=389\n",
            ">46/100, 10/81, d1=0.673, d2=0.705, g=0.736, secs=1543, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=389\n",
            ">46/100, 15/81, d1=0.670, d2=0.707, g=0.749, secs=1545, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=390\n",
            ">46/100, 20/81, d1=0.686, d2=0.703, g=0.750, secs=1547, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=391\n",
            ">46/100, 25/81, d1=0.697, d2=0.676, g=0.745, secs=1549, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=391\n",
            ">46/100, 30/81, d1=0.686, d2=0.673, g=0.760, secs=1551, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=392\n",
            ">46/100, 35/81, d1=0.660, d2=0.676, g=0.760, secs=1554, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=392\n",
            ">46/100, 40/81, d1=0.683, d2=0.691, g=0.751, secs=1556, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=393\n",
            ">46/100, 45/81, d1=0.646, d2=0.694, g=0.763, secs=1558, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=393\n",
            ">46/100, 50/81, d1=0.672, d2=0.689, g=0.754, secs=1560, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=394\n",
            ">46/100, 55/81, d1=0.672, d2=0.695, g=0.760, secs=1562, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=394\n",
            ">46/100, 60/81, d1=0.656, d2=0.694, g=0.757, secs=1564, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=395\n",
            ">46/100, 65/81, d1=0.653, d2=0.704, g=0.741, secs=1566, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=396\n",
            ">46/100, 70/81, d1=0.678, d2=0.697, g=0.750, secs=1568, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=396\n",
            ">46/100, 75/81, d1=0.661, d2=0.690, g=0.757, secs=1570, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=397\n",
            ">46/100, 80/81, d1=0.672, d2=0.682, g=0.748, secs=1572, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=397\n",
            ">47/100, 5/81, d1=0.666, d2=0.674, g=0.753, secs=1575, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=398\n",
            ">47/100, 10/81, d1=0.680, d2=0.683, g=0.745, secs=1577, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=398\n",
            ">47/100, 15/81, d1=0.654, d2=0.698, g=0.742, secs=1579, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=399\n",
            ">47/100, 20/81, d1=0.617, d2=0.714, g=0.726, secs=1581, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=400\n",
            ">47/100, 25/81, d1=0.645, d2=0.722, g=0.726, secs=1583, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=400\n",
            ">47/100, 30/81, d1=0.658, d2=0.710, g=0.734, secs=1585, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=401\n",
            ">47/100, 35/81, d1=0.651, d2=0.726, g=0.736, secs=1587, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=401\n",
            ">47/100, 40/81, d1=0.667, d2=0.705, g=0.743, secs=1589, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=402\n",
            ">47/100, 45/81, d1=0.668, d2=0.714, g=0.738, secs=1591, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=402\n",
            ">47/100, 50/81, d1=0.646, d2=0.691, g=0.733, secs=1594, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=403\n",
            ">47/100, 55/81, d1=0.653, d2=0.702, g=0.741, secs=1596, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=403\n",
            ">47/100, 60/81, d1=0.667, d2=0.699, g=0.754, secs=1598, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=404\n",
            ">47/100, 65/81, d1=0.656, d2=0.697, g=0.744, secs=1600, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=405\n",
            ">47/100, 70/81, d1=0.638, d2=0.707, g=0.742, secs=1602, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=405\n",
            ">47/100, 75/81, d1=0.655, d2=0.705, g=0.728, secs=1604, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=406\n",
            ">47/100, 80/81, d1=0.642, d2=0.717, g=0.715, secs=1606, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=406\n",
            ">48/100, 5/81, d1=0.641, d2=0.717, g=0.727, secs=1609, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=407\n",
            ">48/100, 10/81, d1=0.665, d2=0.712, g=0.731, secs=1611, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=407\n",
            ">48/100, 15/81, d1=0.658, d2=0.724, g=0.731, secs=1613, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=408\n",
            ">48/100, 20/81, d1=0.646, d2=0.708, g=0.727, secs=1615, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=409\n",
            ">48/100, 25/81, d1=0.650, d2=0.710, g=0.734, secs=1617, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=409\n",
            ">48/100, 30/81, d1=0.673, d2=0.689, g=0.743, secs=1619, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=410\n",
            ">48/100, 35/81, d1=0.675, d2=0.702, g=0.741, secs=1621, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=410\n",
            ">48/100, 40/81, d1=0.687, d2=0.701, g=0.739, secs=1623, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=411\n",
            ">48/100, 45/81, d1=0.654, d2=0.704, g=0.751, secs=1625, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=411\n",
            ">48/100, 50/81, d1=0.668, d2=0.699, g=0.736, secs=1627, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=412\n",
            ">48/100, 55/81, d1=0.670, d2=0.684, g=0.742, secs=1629, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=412\n",
            ">48/100, 60/81, d1=0.660, d2=0.697, g=0.739, secs=1631, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=413\n",
            ">48/100, 65/81, d1=0.659, d2=0.692, g=0.746, secs=1634, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=414\n",
            ">48/100, 70/81, d1=0.643, d2=0.707, g=0.741, secs=1636, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=414\n",
            ">48/100, 75/81, d1=0.647, d2=0.709, g=0.750, secs=1638, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=415\n",
            ">48/100, 80/81, d1=0.663, d2=0.685, g=0.744, secs=1640, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=415\n",
            ">49/100, 5/81, d1=0.644, d2=0.705, g=0.731, secs=1642, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=416\n",
            ">49/100, 10/81, d1=0.651, d2=0.697, g=0.736, secs=1644, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=416\n",
            ">49/100, 15/81, d1=0.633, d2=0.707, g=0.731, secs=1646, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=417\n",
            ">49/100, 20/81, d1=0.648, d2=0.733, g=0.732, secs=1649, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=418\n",
            ">49/100, 25/81, d1=0.655, d2=0.707, g=0.739, secs=1651, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=418\n",
            ">49/100, 30/81, d1=0.663, d2=0.694, g=0.737, secs=1653, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=419\n",
            ">49/100, 35/81, d1=0.643, d2=0.707, g=0.736, secs=1655, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=419\n",
            ">49/100, 40/81, d1=0.667, d2=0.703, g=0.736, secs=1657, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=420\n",
            ">49/100, 45/81, d1=0.661, d2=0.705, g=0.763, secs=1659, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=420\n",
            ">49/100, 50/81, d1=0.673, d2=0.682, g=0.757, secs=1661, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=421\n",
            ">49/100, 55/81, d1=0.662, d2=0.693, g=0.744, secs=1663, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=421\n",
            ">49/100, 60/81, d1=0.688, d2=0.686, g=0.748, secs=1665, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=422\n",
            ">49/100, 65/81, d1=0.678, d2=0.688, g=0.733, secs=1667, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=423\n",
            ">49/100, 70/81, d1=0.681, d2=0.705, g=0.752, secs=1669, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=423\n",
            ">49/100, 75/81, d1=0.651, d2=0.700, g=0.734, secs=1671, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=424\n",
            ">49/100, 80/81, d1=0.658, d2=0.704, g=0.730, secs=1673, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=424\n",
            ">50/100, 5/81, d1=0.668, d2=0.705, g=0.746, secs=1676, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=425\n",
            ">50/100, 10/81, d1=0.668, d2=0.686, g=0.769, secs=1678, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=425\n",
            ">50/100, 15/81, d1=0.677, d2=0.680, g=0.758, secs=1680, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=426\n",
            ">50/100, 20/81, d1=0.652, d2=0.697, g=0.739, secs=1682, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=427\n",
            ">50/100, 25/81, d1=0.666, d2=0.711, g=0.749, secs=1684, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=427\n",
            ">50/100, 30/81, d1=0.663, d2=0.706, g=0.751, secs=1686, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=428\n",
            ">50/100, 35/81, d1=0.659, d2=0.689, g=0.748, secs=1688, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=428\n",
            ">50/100, 40/81, d1=0.649, d2=0.692, g=0.767, secs=1691, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=429\n",
            ">50/100, 45/81, d1=0.665, d2=0.701, g=0.761, secs=1693, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=429\n",
            ">50/100, 50/81, d1=0.681, d2=0.686, g=0.757, secs=1695, tryAgain=0, nTripsOnSameSavedWts=0, nSaves=430\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/xray_ /content/drive/MyDrive/cGAN_xray"
      ],
      "metadata": {
        "id": "9Sb7OOKQ-rz3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/drive/MyDrive/cGAN_xray/xray/real_plots"
      ],
      "metadata": {
        "id": "FVwqJJPNBRBh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-eoxwjfYOjK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inferernce model----------------"
      ],
      "metadata": {
        "id": "LwZ6R96DOjHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a gan for generating faces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import load\n",
        "from numpy import zeros\n",
        "from numpy import ones\n",
        "from numpy import asarray\n",
        "from numpy import append\n",
        "from numpy.random import random\n",
        "from numpy.random import randint\n",
        "from numpy.random import shuffle\n",
        "import time\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adamax\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Conv2DTranspose\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from matplotlib import patheffects as path_effects\n",
        "import collections\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow import get_logger as log\n",
        "\n",
        "\n",
        "# generate points in latent space as input for the generator\n",
        "def generate_latent_points(latent_dim, n_samples, cumProbs, n_classes=3):\n",
        "\t# print(\"generate_latent_points: \", latent_dim, n_samples)\n",
        "\tinitX = -3.0\n",
        "\trangeX = 2.0*abs(initX)\n",
        "\tstepX = rangeX / (latent_dim * n_samples)\n",
        "\tx_input = asarray([initX + stepX*(float(i)) for i in range(0,latent_dim * n_samples)])\n",
        "\tshuffle(x_input)\n",
        "\t# generate points in the latent space\n",
        "\tz_input = x_input.reshape(n_samples, latent_dim)\n",
        "\trandx = random(n_samples)\n",
        "\tlabels = np.zeros(n_samples, dtype=int)\n",
        "\tfor i in range(n_classes):\n",
        "\t\tlabels = np.where((randx >= cumProbs[i]) & (randx < cumProbs[i+1]), i, labels)\n",
        "\treturn [z_input, labels]\n",
        " \n",
        "# use the generator to generate n fake examples, with class labels\n",
        "def generate_fake_samples(generator, latent_dim, n_samples, cumProbs):\n",
        "\t# generate points in latent space\n",
        "\tz_input, labels_input = generate_latent_points(latent_dim, n_samples, cumProbs)\n",
        "\t# predict outputs\n",
        "\timages = generator.predict([z_input, labels_input])\n",
        "\t# create class labels\n",
        "\ty = zeros((n_samples, 1))\n",
        "\treturn [images, labels_input], y\n",
        " \n",
        "\"\"\"\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, labels, epoch, n=10):\n",
        "\t# scale from [-1,1] to [0,1]\n",
        "\texamples = (examples + 1) / 2.0\n",
        "\t# plot images\n",
        "\tfor i in range(n * n):\n",
        "\t\t# define subplot\n",
        "\t\tfig = plt.subplot(n, n, 1 + i)\n",
        "\t\tstrLabel = str(labels[i])\n",
        "\t\t# turn off axis\n",
        "\t\tfig.axis('off')\n",
        "\t\tfig.text(8.0,20.0,strLabel, fontsize=6, color='white')\n",
        "\t\t# plot raw pixel data\n",
        "\t\tfig.imshow(examples[i])\n",
        "\t# save plot to file\n",
        "\tfilename = './generated_plot_e%03d.png' % (epoch+1)\n",
        "\tplt.savefig(filename)\n",
        "\tplt.close()\n",
        " \n",
        " \"\"\"\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, labels, epoch, n=3):\n",
        "    \n",
        "    varNames = [\"NORMAL\",\"VIRAL\",\"BACTERIAL\"]\n",
        "\n",
        "    # scale from [-1,1] to [0,1]\n",
        "    fig, ax = plt.subplots(n, n, figsize=(15,15))\n",
        "    examples = (examples + 1) / 2.0\n",
        "    ax=ax.flatten()\n",
        "    # plot images\n",
        "    for i in range(n * n):\n",
        "        # define subplot\n",
        "        #fig = plt.subplot(n, n, 1 + i)\n",
        "        strLabel = str(varNames[labels[i]])\n",
        "        # turn off axis\n",
        "        #fig.axis('off')\n",
        "        #fig.text(8.0,20.0,strLabel, fontsize=6, color='white')\n",
        "        # plot raw pixel data\n",
        "        #fig.imshow(examples[i])\n",
        "        ax[i].set_title(strLabel)\n",
        "        # plot raw pixel data\n",
        "        ax[i].imshow(examples[i])\n",
        "    # save plot to file\n",
        "    filename = './generated_plot_e%03d.png' % (epoch+1)\n",
        "    plt.savefig(filename)\n",
        "    plt.close()\n",
        " \n",
        "\n",
        " \n",
        "# evaluate the discriminator, plot generated images, save generator model\n",
        "def summarize_performance(epoch, g_model, d_model, latent_dim,cumProbs, n_samples=100):\n",
        "\t# prepare fake examples\n",
        "\t[X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, n_samples, cumProbs)\n",
        "\t# evaluate discriminator on fake examples\n",
        "\t_, acc_fake = d_model.evaluate([X_fake, labels], y_fake, verbose=0)\n",
        "\t# summarize discriminator performance\n",
        "\t#print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\t# save plot\n",
        "\tsave_plot(X_fake, labels, epoch)\n",
        " \n",
        "\n",
        "\n",
        "g =load_model(\"/content/drive/MyDrive/cGAN_xray/xray/results/generator_model_040.h5\")\n",
        "d =load_model(\"/content/drive/MyDrive/cGAN_xray/xray/results/generator_model_dis040.h5\")\n",
        "latent_dim = 100\n",
        "cumProbs = [0.,         0.2696918,  0.52534249, 1.00000003]\n",
        "summarize_performance(111, g, d,latent_dim, cumProbs, n_samples=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnU3NqIXCT_y",
        "outputId": "93f7a449-5aef-4e80-ba03-a75ef8196ea5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1260933b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f12608463b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M5LUgnjXKxm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EuzCrieKxkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "varNames = [\"NORMAL\",\"VIRAL\",\"BACTERIAL\"]\n",
        "def save_real_plots(dataset, nRealPlots = 5, n=3, n_samples=100):\n",
        "    # plot images\n",
        "    for epoch in range(nRealPlots):\n",
        "        if epoch%5==0:\n",
        "            print(\"real_plots: \", epoch)\n",
        "        # prepare real samples\n",
        "        [X_real, labels], y_real = generate_real_samples(dataset, 100)\n",
        "        fig, ax = plt.subplots(n, n, figsize=(15,15))\n",
        "        ax=ax.flatten()\n",
        "        # scale from [-1,1] to [0,1]\n",
        "        X_real = (X_real + 1) / 2.0\n",
        "        for i in range(n * n):\n",
        "            # define subplot\n",
        "            #fig = plt.subplot(n, n, 1 + i)\n",
        "            strLabel = str(varNames[labels[i]])\n",
        "            print(\"strLabel :\", strLabel)\n",
        "            # fig.title = strLabel\n",
        "            # turn off axis\n",
        "            #fig.axis('off')\n",
        "            #fig.text(8.0,20.0,strLabel, fontsize=6, color='blue')\n",
        "            ax[i].set_title(strLabel)\n",
        "            # plot raw pixel data\n",
        "            ax[i].imshow(X_real[i])\n",
        "        # save plot to file\n",
        "        filename = 'xray/test_/real_plot_e%03d.png' % (epoch+1)\n",
        "        plt.savefig(filename)\n",
        "        plt.close()\n",
        "        "
      ],
      "metadata": {
        "id": "zs0dD0sPKxh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}